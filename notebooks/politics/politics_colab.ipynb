{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NancyFulda/towards-neural-programming-interfaces/blob/colab/notebooks/politics/politics_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDrpvhHvEiBm"
      },
      "source": [
        "# Python Notebook training NPI on political views\n",
        "\n",
        "This notebook is also an example on how to use the NPI project to train your own NPI in a way that can affect GPT2's sentiment or emotion. This notebook attempts to train the GPT2 model with NPI to possess a specific political view."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2wEFrcBEiBq"
      },
      "source": [
        "## Setup\n",
        "\n",
        "### Prerequisite setup:\n",
        "\n",
        "*   Ensure you selected a runtime with a GPU. You can do that by clicking Runtime -> Change runtime type ->  Hardware Accelerator -> GPU\n",
        "*   After running the setup code in this section, restart the runtime by clicking Runtime -> Restart runtime.\n",
        "\n",
        "After you have done the prerequisite setup, you will not need to rerun this section again.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd towards-neural-programming-interfaces && git pull"
      ],
      "metadata": {
        "id": "Avd8wJxxp1Xw",
        "outputId": "aa199a2a-ecb9-4090-ca7c-312b6bdf33c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Counting objects:   7% (1/13)\u001b[K\rremote: Counting objects:  15% (2/13)\u001b[K\rremote: Counting objects:  23% (3/13)\u001b[K\rremote: Counting objects:  30% (4/13)\u001b[K\rremote: Counting objects:  38% (5/13)\u001b[K\rremote: Counting objects:  46% (6/13)\u001b[K\rremote: Counting objects:  53% (7/13)\u001b[K\rremote: Counting objects:  61% (8/13)\u001b[K\rremote: Counting objects:  69% (9/13)\u001b[K\rremote: Counting objects:  76% (10/13)\u001b[K\rremote: Counting objects:  84% (11/13)\u001b[K\rremote: Counting objects:  92% (12/13)\u001b[K\rremote: Counting objects: 100% (13/13)\u001b[K\rremote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects:  50% (1/2)\u001b[K\rremote: Compressing objects: 100% (2/2)\u001b[K\rremote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 13 (delta 9), reused 13 (delta 9), pack-reused 0\u001b[K\n",
            "Unpacking objects:   7% (1/13)   \rUnpacking objects:  15% (2/13)   \rUnpacking objects:  23% (3/13)   \rUnpacking objects:  30% (4/13)   \rUnpacking objects:  38% (5/13)   \rUnpacking objects:  46% (6/13)   \rUnpacking objects:  53% (7/13)   \rUnpacking objects:  61% (8/13)   \rUnpacking objects:  69% (9/13)   \rUnpacking objects:  76% (10/13)   \rUnpacking objects:  84% (11/13)   \rUnpacking objects:  92% (12/13)   \rUnpacking objects: 100% (13/13)   \rUnpacking objects: 100% (13/13), done.\n",
            "From https://github.com/NancyFulda/towards-neural-programming-interfaces\n",
            "   1771ff3..8443bb7  colab      -> origin/colab\n",
            "Updating 1771ff3..8443bb7\n",
            "Fast-forward\n",
            " install_dependencies.sh                      |  4 \u001b[31m----\u001b[m\n",
            " notebooks/politics/politics.ipynb            | 17 \u001b[32m+++++++++++\u001b[m\u001b[31m------\u001b[m\n",
            " src/npi/dataset/construct_dataset.py         |  2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " src/npi/dataset/npi_dataset.py               |  8 \u001b[32m++++\u001b[m\u001b[31m----\u001b[m\n",
            " src/npi/training/style_classifier_trainer.py |  3 \u001b[31m---\u001b[m\n",
            " 5 files changed, 16 insertions(+), 18 deletions(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NancyFulda/towards-neural-programming-interfaces.git\n",
        "!cd towards-neural-programming-interfaces && ./install_dependencies.sh"
      ],
      "metadata": {
        "id": "8tnQFi2DEqge",
        "outputId": "1038825e-4e58-48f3-cdff-42c628f73550",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'towards-neural-programming-interfaces'...\n",
            "remote: Enumerating objects: 427, done.\u001b[K\n",
            "remote: Counting objects: 100% (356/356), done.\u001b[K\n",
            "remote: Compressing objects: 100% (253/253), done.\u001b[K\n",
            "remote: Total 427 (delta 194), reused 218 (delta 83), pack-reused 71\u001b[K\n",
            "Receiving objects: 100% (427/427), 305.29 MiB | 14.21 MiB/s, done.\n",
            "Resolving deltas: 100% (216/216), done.\n",
            "Checking out files: 100% (60/60), done.\n",
            "Branch 'colab' set up to track remote branch 'colab' from 'origin'.\n",
            "Switched to a new branch 'colab'\n",
            "installing packages\n",
            "Obtaining file:///content/towards-neural-programming-interfaces (from -r requirements.txt (line 2))\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.21.5)\n",
            "Collecting torch==1.5.1\n",
            "  Downloading torch-1.5.1-cp37-cp37m-manylinux1_x86_64.whl (753.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 753.2 MB 14 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.1\n",
            "  Downloading torchvision-0.6.1-cp37-cp37m-manylinux1_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 14.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.4.1)\n",
            "Collecting webdataset==0.1.*\n",
            "  Downloading webdataset-0.1.103-py3-none-any.whl (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (1.15.0)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.21.28-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 43.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (4.63.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 44.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 17)) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 18)) (1.0.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 21)) (2.2.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.5.1->-r requirements.txt (line 5)) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.6.1->-r requirements.txt (line 6)) (7.1.2)\n",
            "Collecting braceexpand\n",
            "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from webdataset==0.1.*->-r requirements.txt (line 8)) (3.13)\n",
            "Collecting botocore<1.25.0,>=1.24.28\n",
            "  Downloading botocore-1.24.28-py3-none-any.whl (8.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.6 MB 43.6 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.4 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 48.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.28->boto3->-r requirements.txt (line 12)) (2.8.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 13)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 13)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 13)) (2021.10.8)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 35.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->-r requirements.txt (line 16)) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->-r requirements.txt (line 16)) (7.1.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 17)) (1.4.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 17)) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 17)) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->-r requirements.txt (line 17)) (3.10.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 18)) (3.1.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 21)) (1.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 21)) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 21)) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 21)) (2.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 21)) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 21)) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 21)) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 21)) (0.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 21)) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 21)) (3.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->-r requirements.txt (line 21)) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy->-r requirements.txt (line 21)) (3.7.0)\n",
            "Installing collected packages: urllib3, jmespath, botocore, torch, s3transfer, braceexpand, webdataset, torchvision, sacremoses, npi, boto3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.11.1+cu111\n",
            "    Uninstalling torchvision-0.11.1+cu111:\n",
            "      Successfully uninstalled torchvision-0.11.1+cu111\n",
            "  Running setup.py develop for npi\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.5.1 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.5.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.21.28 botocore-1.24.28 braceexpand-0.1.7 jmespath-1.0.0 npi-0.1.0 s3transfer-0.5.2 sacremoses-0.0.49 torch-1.5.1 torchvision-0.6.1 urllib3-1.25.11 webdataset-0.1.103\n",
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.21.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.63.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "unzip transformers directory and corpus\n",
            "Archive:  transformers.zip\n",
            "   creating: src/npi/transformers/\n",
            "  inflating: src/npi/transformers/modeling_tf_xlnet.py  \n",
            "  inflating: src/npi/transformers/tokenization_xlm.py  \n",
            "  inflating: src/npi/transformers/tokenization_xlnet.py  \n",
            "  inflating: src/npi/transformers/modeling_tf_pytorch_utils.py  \n",
            "  inflating: src/npi/transformers/configuration_utils.py  \n",
            "  inflating: src/npi/transformers/gan_data.py  \n",
            "  inflating: src/npi/transformers/modeling_tf_roberta.py  \n",
            "  inflating: src/npi/transformers/modeling_openai.py  \n",
            "  inflating: src/npi/transformers/tokenization_distilbert.py  \n",
            "  inflating: src/npi/transformers/modeling_tf_transfo_xl.py  \n",
            "  inflating: src/npi/transformers/configuration_transfo_xl.py  \n",
            "  inflating: src/npi/transformers/tokenization_gpt2.py  \n",
            "  inflating: src/npi/transformers/modeling_xlnet.py  \n",
            "  inflating: src/npi/transformers/modeling_bert.py  \n",
            "  inflating: src/npi/transformers/tokenization_transfo_xl.py  \n",
            "  inflating: src/npi/transformers/modeling_tf_distilbert.py  \n",
            "  inflating: src/npi/transformers/configuration_distilbert.py  \n",
            "  inflating: src/npi/transformers/configuration_gpt2.py  \n",
            "  inflating: src/npi/transformers/modeling_tf_gpt2.py  \n",
            "  inflating: src/npi/transformers/modeling_tf_transfo_xl_utilities.py  \n",
            "  inflating: src/npi/transformers/tokenization_roberta.py  \n",
            "  inflating: src/npi/transformers/configuration_openai.py  \n",
            "  inflating: src/npi/transformers/__main__.py  \n",
            "  inflating: src/npi/transformers/modeling_tf_bert.py  \n",
            "  inflating: src/npi/transformers/modeling_roberta.py  \n",
            "  inflating: src/npi/transformers/modeling_transfo_xl_utilities.py  \n",
            "  inflating: src/npi/transformers/configuration_roberta.py  \n",
            "   creating: src/npi/transformers/__pycache__/\n",
            "  inflating: src/npi/transformers/__pycache__/configuration_gpt2.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/modeling_openai.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/modeling_distilbert.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/configuration_openai.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/configuration_utils.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/modeling_roberta.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/tokenization_bert.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/gan_data.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/__init__.cpython-37.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/configuration_xlm.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/tokenization_openai.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/run_generation.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/configuration_auto.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/configuration_transfo_xl.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/modeling_xlm.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/tokenization_xlnet.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/tokenization_transfo_xl.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/modeling_xlnet.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/modeling_bert.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/tokenization_xlm.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/configuration_distilbert.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/modeling_transfo_xl_utilities.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/modeling_gpt2.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/tokenization_auto.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/file_utils.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/tokenization_distilbert.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/configuration_xlnet.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/modeling_utils.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/tokenization_utils.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/modeling_auto.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/tokenization_gpt2.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/modeling_transfo_xl.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/configuration_roberta.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/optimization.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/tokenization_roberta.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/configuration_bert.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/__pycache__/file_utils.cpython-37.pyc  \n",
            "  inflating: src/npi/transformers/configuration_bert.py  \n",
            "  inflating: src/npi/transformers/modeling_xlm.py  \n",
            "  inflating: src/npi/transformers/modeling_gpt2.py  \n",
            "  inflating: src/npi/transformers/tokenization_bert.py  \n",
            "  inflating: src/npi/transformers/run_generation.py  \n",
            "  inflating: src/npi/transformers/modeling_tf_auto.py  \n",
            "   creating: src/npi/transformers/data/\n",
            "  inflating: src/npi/transformers/data/__init__.py  \n",
            "   creating: src/npi/transformers/data/metrics/\n",
            "   creating: src/npi/transformers/data/metrics/__pycache__/\n",
            "  inflating: src/npi/transformers/data/metrics/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/data/metrics/__init__.py  \n",
            "   creating: src/npi/transformers/data/__pycache__/\n",
            "  inflating: src/npi/transformers/data/__pycache__/__init__.cpython-36.pyc  \n",
            "   creating: src/npi/transformers/data/processors/\n",
            "   creating: src/npi/transformers/data/processors/__pycache__/\n",
            "  inflating: src/npi/transformers/data/processors/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/data/processors/__pycache__/glue.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/data/processors/__pycache__/utils.cpython-36.pyc  \n",
            "  inflating: src/npi/transformers/data/processors/utils.py  \n",
            "  inflating: src/npi/transformers/data/processors/glue.py  \n",
            "  inflating: src/npi/transformers/data/processors/__init__.py  \n",
            "  inflating: src/npi/transformers/tokenization_auto.py  \n",
            "  inflating: src/npi/transformers/modeling_distilbert.py  \n",
            "  inflating: src/npi/transformers/configuration_auto.py  \n",
            "  inflating: src/npi/transformers/modeling_auto.py  \n",
            "  inflating: src/npi/transformers/configuration_xlm.py  \n",
            "  inflating: src/npi/transformers/modeling_tf_xlm.py  \n",
            "  inflating: src/npi/transformers/optimization.py  \n",
            "  inflating: src/npi/transformers/file_utils.py  \n",
            "  inflating: src/npi/transformers/modeling_utils.py  \n",
            "  inflating: src/npi/transformers/tokenization_openai.py  \n",
            "  inflating: src/npi/transformers/configuration_xlnet.py  \n",
            "  inflating: src/npi/transformers/tokenization_utils.py  \n",
            "  inflating: src/npi/transformers/modeling_transfo_xl.py  \n",
            "  inflating: src/npi/transformers/modeling_tf_openai.py  \n",
            "  inflating: src/npi/transformers/__init__.py  \n",
            "  inflating: src/npi/transformers/modeling_tf_utils.py  \n",
            "Archive:  data/raw/small_corpus.zip\n",
            "  inflating: smaller_wiki_books_reddit_shuffled.txt  \n",
            "mv: cannot stat 'transformers/run_generation.py': No such file or directory\n",
            "make necessary directories\n",
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the NewB news source dataset\n",
        "!git clone https://github.com/JerryWei03/NewB.git"
      ],
      "metadata": {
        "id": "uuOj-3SyFNBj",
        "outputId": "f1a10dbd-f8e4-4a73-a78b-dbe532c2f9a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NewB'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 14 (delta 1), reused 4 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (14/14), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post setup\n",
        "\n",
        "***Note: You will need to restart the runtime on Colab to continue***\n",
        "\n",
        "Click on Runtime -> Restart Runtime to do so.\n"
      ],
      "metadata": {
        "id": "7-Q2yk4kGLnX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8vYjqYt4EiBs"
      },
      "outputs": [],
      "source": [
        "# For dev purposes, enable autoreload of modules\n",
        "%load_ext autoreload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6AxvkhH3EiBu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "%autoreload\n",
        "\n",
        "# Constructing training dataset\n",
        "from npi.dataset import NPIDatasetConstructor, NPIDatasetLoader\n",
        "\n",
        "# Models\n",
        "from npi.models import NPITrainingModels\n",
        "\n",
        "# Training NPI\n",
        "from npi.training import NPIStyleTrainer, NPITrainer\n",
        "\n",
        "# Configuration\n",
        "from npi.config import NPIConfig\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that each step in training an NPI is conditioned on the variable toggles, so you can disable certain steps to not redo them again when running the whole notebook."
      ],
      "metadata": {
        "id": "PBynmrGsJqt3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nV6-eOMoEiBu"
      },
      "outputs": [],
      "source": [
        "# Toggle steps\n",
        "construct = True\n",
        "train_class = True\n",
        "test_class = True\n",
        "train_npi = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Xr_mUYVNEiBv"
      },
      "outputs": [],
      "source": [
        "model_layers = [5, 11]\n",
        "\n",
        "device = torch.device(f\"cuda:0\")\n",
        "config = NPIConfig(\n",
        "    device, gpt_model=\"gpt2\", perturbation_indices=model_layers, npi_name=\"politics\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdqqaFZCEiBv"
      },
      "source": [
        "## Create Dataset\n",
        "\n",
        "### Get Data from NewB GitHub repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtXZG3B1EiBw"
      },
      "source": [
        "See `construct_politics_data` to see how to construct data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YubiuraEiBx"
      },
      "outputs": [],
      "source": [
        "if construct:\n",
        "    with open(\"./NewB/train_orig.txt\", \"r\", newline=\"\") as f:\n",
        "        data = pd.read_csv(f, delimiter=\"\\t\", names=[\"view\", \"text\"])\n",
        "        data[\"view\"] = data[\"view\"].replace(\n",
        "            to_replace={0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1}\n",
        "        )\n",
        "        data = data[data.view != 5]\n",
        "        data = data.sample(frac=1)\n",
        "        data_iter = zip(data[\"text\"], data[\"view\"])\n",
        "        construct_data = NPIDatasetConstructor(config)\n",
        "        construct_data.construct_dataset(data_iter, 4000) # Only creating 4000 training data points\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXWpcynGEiBx"
      },
      "source": [
        "## Train classifier from Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgooQZnZEiBy"
      },
      "outputs": [],
      "source": [
        "model_loader = NPITrainingModels(config)\n",
        "dataset_loader = NPIDatasetLoader(config, target=\"style\")\n",
        "style_trainer = NPIStyleTrainer(config, class_lr=1e-5)\n",
        "classifier = None\n",
        "if train_class:\n",
        "    classifier = style_trainer.train_classifier(\n",
        "        model_loader,\n",
        "        dataset_loader,\n",
        "        num_epochs=5,\n",
        "        #continue_epoch=4,  # Set epoch of classifier to load to continue training.\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz2o83DCEiBy"
      },
      "source": [
        "### Test Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkTIcEWnEiBy",
        "outputId": "78df1654-0d8f-43ee-e986-e795bd53f4fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classifier INIT\n",
            "Defining classifier model\n",
            "Loading style_model weights from models/npi_models/politics_style_model_008.pth\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(69738789.6, 0.44000000000000006)"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "_, test_loader, _, _ = dataset_loader.load_train_and_test_dataloaders()\n",
        "\n",
        "classifier = model_loader.load_style_classifier() if not classifier else classifier\n",
        "# When output=True, it will log the following in models/npi_models/style_classifier_output.log:\n",
        "#   (generated text from the dataset)\n",
        "#   truth={target_label} actual={model_output}\n",
        "style_trainer.test_model(test_loader, classifier, torch.nn.BCELoss(), output=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH-dlXcQEiBz"
      },
      "source": [
        "## Training NPI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8qQUlUbEiBz"
      },
      "outputs": [],
      "source": [
        "# Initialize models for training and testing\n",
        "models = NPITrainingModels(\n",
        "    config,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6oi0MflEiB0"
      },
      "outputs": [],
      "source": [
        "num_epochs = 6\n",
        "split_ratio = 0.25\n",
        "batch_size = 5\n",
        "headstart = 0  # set at 0 for debugging\n",
        "\n",
        "if train_npi:\n",
        "    trainer = NPITrainer(config, batch_size=batch_size, headstart=headstart)\n",
        "    dataset_loader = NPIDatasetLoader(config, split_ratio=split_ratio)\n",
        "    trainer.train_adversarial_npi(models, num_epochs, dataset_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwD1OEyuEiB0"
      },
      "source": [
        "## Test NPI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M76gOLwSEiB0"
      },
      "source": [
        "Using the model loader, you can directly generate NPI text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dINaz8IEiB0",
        "outputId": "979a2668-d018-46e9-a260-eb1efbcc5924"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing GPT2WithNPI model with tokenizer -- not being placed on GPU until npi loss evaluation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                       "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Original GPT2 output: 'Trump today was found'\n",
            " to have been a liar.\n",
            "\n",
            "The FBI has been investigating the Trump campaign's ties to Russia.\n",
            "\n",
            "The Trump campaign has also been accused of using the word \"radical\" in a tweet.\n",
            "\n",
            "\"I am not a fan of the idea of a \"big-screen\" movie.\n",
            "\n",
            "\"I'm not going to say that I'm going to say that I'm going to say that I'm going to say that I'm going to say that I'm going to say that I'm going to say\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "text = \"Trump today was found\"\n",
        "print(F\"Original GPT2 output: '{text}'\\n{models.gpt2_generate_text(text)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqAxiIa_EiB1",
        "outputId": "9fa13d3d-0c9c-4f40-9634-fecf5012bf5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NPI INITIALIZATION\n",
            "Loading npi_model weights from models/npi_models/politics_npi_model_000.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                     "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NPI GPT2 output: 'Trump today was found'\n",
            " to have been a liar.\n",
            "\n",
            "The FBI has been investigating the Trump campaign's ties to Russia.\n",
            "\n",
            "The Trump campaign has also been accused of using the word \"radical\" in a tweet.\n",
            "\n",
            "\"I am not a racist, I am not a racist, I am not a racist, I am not a racist, I am not a racist, I am not a racist, I am not a racist, I am not a racist, I am not a racist, I am not a racist, I am\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "print(F\"NPI GPT2 output: '{text}'\\n{models.npi_generate_text(text)}\")"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "b53feeeef0abc6b71390ce5a6fde0c401718055bbb2d86de1b9dab62a8ba524e"
    },
    "kernelspec": {
      "display_name": "Python 3.8.0 64-bit ('towards-neural-programming-interfaces-UdnRbY7B': pipenv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "politics.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}