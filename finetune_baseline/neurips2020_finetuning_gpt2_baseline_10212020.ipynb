{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "neurips2020_finetuning_gpt2_baseline_10212020.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "thN1bHGvpiMt",
        "outputId": "e15f2b72-295e-44ee-a3b8-0afdfd1b5826",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "!pip3 install gpt-2-simple\n",
        "%tensorflow_version 1.14"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gpt-2-simple\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/e4/a90add0c3328eed38a46c3ed137f2363b5d6a07bf13ee5d5d4d1e480b8c3/gpt_2_simple-0.7.1.tar.gz\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (1.18.5)\n",
            "Collecting toposort\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (2.10)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.7.1-cp36-none-any.whl size=23581 sha256=5dd7861fb690fa4f6d88e0f2e09793aa8441aaa87e17fe98c3e8d106bde41f47\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/f8/23/b53ce437504597edff76bf9c3b8de08ad716f74f6c6baaa91a\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.7.1 toposort-1.5\n",
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.14`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydGjKjXFppMF"
      },
      "source": [
        "\"\"\"based on https://github.com/minimaxir/gpt-2-simple/blob/master/gpt_2_simple/gpt_2.py\"\"\"\n",
        "\n",
        "import gpt_2_simple as gpt2                                                       \n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "def run_fine_tune():\n",
        "    # This line is necessary to be able to run a new tf session\n",
        "    tf.reset_default_graph()\n",
        "    # The medium-sized model. IF you run out of memory, try \"124M\" instead\n",
        "    model_name = \"124M\"\n",
        "    if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
        "        print(f\"Downloading {model_name} model...\")\n",
        "        gpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/124M/\n",
        "    \n",
        "    # Start a session\n",
        "    sess = gpt2.start_tf_sess()\n",
        "    # Fine tune `model_name` on `data`\n",
        "    ###################################\n",
        "    # Swap out the `dataset` parameter with the path to your text dataset\n",
        "    ###################################\n",
        "    gpt2.finetune(sess,\n",
        "                  dataset='gpt2_finetuned_baseline_data_combined_trainTest_elem5_strings_ALL.txt',\n",
        "                 steps=1500000000000000000000000,\n",
        "                 model_name='124M',\n",
        "                 model_dir='models',\n",
        "                 combine=50000,\n",
        "                 batch_size=1,\n",
        "                 learning_rate=0.0001,\n",
        "                 accumulate_gradients=5,\n",
        "                 restore_from='latest',\n",
        "                 run_name='run1',\n",
        "                 checkpoint_dir='checkpoint',\n",
        "                 sample_every=100,\n",
        "                 sample_length=100,\n",
        "                 sample_num=1,\n",
        "                 multi_gpu=False,\n",
        "                 save_every=15,\n",
        "                 print_every=1,\n",
        "                 max_checkpoints=10000,\n",
        "                 use_memory_saving_gradients=False,\n",
        "                 only_train_transformer_layers=False,\n",
        "                 optimizer='adam',\n",
        "                 overwrite=False)   # steps is max number of training steps\n",
        "\n",
        "    gpt2.generate(sess, run_name='run1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvgJ28SFp2k7",
        "outputId": "2981c1a3-88de-4dd4-88aa-9950a0d2aa73",
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "fine_tune_sess1 = run_fine_tune()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 302Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 101Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 308Mit/s]                                                    "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading 124M model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:04, 122Mit/s]                                   \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 380Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 177Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 197Mit/s]                                                       \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:04<00:00,  4.50s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 1012206 tokens\n",
            "Training...\n",
            "[1 | 7.69] loss=3.92 avg=3.92\n",
            "[2 | 9.91] loss=3.09 avg=3.50\n",
            "[3 | 12.12] loss=2.91 avg=3.30\n",
            "[4 | 14.35] loss=2.66 avg=3.14\n",
            "[5 | 16.58] loss=2.64 avg=3.04\n",
            "[6 | 18.83] loss=2.48 avg=2.94\n",
            "[7 | 21.08] loss=2.58 avg=2.89\n",
            "[8 | 23.32] loss=2.39 avg=2.82\n",
            "[9 | 25.57] loss=2.31 avg=2.76\n",
            "[10 | 27.82] loss=2.32 avg=2.72\n",
            "[11 | 30.08] loss=2.23 avg=2.67\n",
            "[12 | 32.35] loss=2.20 avg=2.63\n",
            "[13 | 34.62] loss=2.16 avg=2.59\n",
            "[14 | 36.89] loss=2.16 avg=2.56\n",
            "[15 | 39.17] loss=1.99 avg=2.52\n",
            "Saving checkpoint/run1/model-15\n",
            "[16 | 44.24] loss=2.03 avg=2.49\n",
            "[17 | 46.53] loss=1.93 avg=2.45\n",
            "[18 | 48.83] loss=2.00 avg=2.42\n",
            "[19 | 51.14] loss=1.93 avg=2.39\n",
            "[20 | 53.46] loss=1.91 avg=2.37\n",
            "[21 | 55.78] loss=1.92 avg=2.34\n",
            "[22 | 58.12] loss=1.92 avg=2.32\n",
            "[23 | 60.46] loss=1.84 avg=2.30\n",
            "[24 | 62.82] loss=1.76 avg=2.27\n",
            "[25 | 65.17] loss=1.82 avg=2.25\n",
            "[26 | 67.54] loss=1.75 avg=2.23\n",
            "[27 | 69.90] loss=1.68 avg=2.21\n",
            "[28 | 72.26] loss=1.62 avg=2.18\n",
            "[29 | 74.62] loss=1.67 avg=2.16\n",
            "[30 | 77.00] loss=1.75 avg=2.15\n",
            "Saving checkpoint/run1/model-30\n",
            "[31 | 81.92] loss=1.64 avg=2.13\n",
            "[32 | 84.31] loss=1.65 avg=2.11\n",
            "[33 | 86.72] loss=1.59 avg=2.09\n",
            "[34 | 89.12] loss=1.67 avg=2.08\n",
            "[35 | 91.54] loss=1.59 avg=2.06\n",
            "[36 | 93.97] loss=1.56 avg=2.05\n",
            "[37 | 96.41] loss=1.58 avg=2.03\n",
            "[38 | 98.87] loss=1.60 avg=2.02\n",
            "[39 | 101.31] loss=1.58 avg=2.00\n",
            "[40 | 103.76] loss=1.61 avg=1.99\n",
            "[41 | 106.24] loss=1.54 avg=1.98\n",
            "[42 | 108.71] loss=1.51 avg=1.96\n",
            "[43 | 111.16] loss=1.61 avg=1.95\n",
            "[44 | 113.64] loss=1.49 avg=1.94\n",
            "[45 | 116.10] loss=1.41 avg=1.93\n",
            "Saving checkpoint/run1/model-45\n",
            "[46 | 120.82] loss=1.43 avg=1.91\n",
            "[47 | 123.27] loss=1.61 avg=1.90\n",
            "[48 | 125.71] loss=1.51 avg=1.89\n",
            "[49 | 128.14] loss=1.46 avg=1.88\n",
            "[50 | 130.58] loss=1.51 avg=1.87\n",
            "[51 | 133.01] loss=1.50 avg=1.86\n",
            "[52 | 135.44] loss=1.46 avg=1.85\n",
            "[53 | 137.88] loss=1.33 avg=1.84\n",
            "[54 | 140.30] loss=1.37 avg=1.83\n",
            "[55 | 142.72] loss=1.47 avg=1.82\n",
            "[56 | 145.15] loss=1.34 avg=1.81\n",
            "[57 | 147.57] loss=1.42 avg=1.80\n",
            "[58 | 149.99] loss=1.40 avg=1.79\n",
            "[59 | 152.41] loss=1.37 avg=1.78\n",
            "[60 | 154.83] loss=1.40 avg=1.78\n",
            "Saving checkpoint/run1/model-60\n",
            "[61 | 159.55] loss=1.40 avg=1.77\n",
            "[62 | 161.94] loss=1.40 avg=1.76\n",
            "[63 | 164.35] loss=1.33 avg=1.75\n",
            "[64 | 166.76] loss=1.31 avg=1.74\n",
            "[65 | 169.18] loss=1.46 avg=1.73\n",
            "[66 | 171.61] loss=1.44 avg=1.73\n",
            "[67 | 174.04] loss=1.40 avg=1.72\n",
            "[68 | 176.48] loss=1.33 avg=1.71\n",
            "[69 | 178.93] loss=1.42 avg=1.71\n",
            "[70 | 181.38] loss=1.34 avg=1.70\n",
            "[71 | 183.83] loss=1.49 avg=1.70\n",
            "[72 | 186.28] loss=1.35 avg=1.69\n",
            "[73 | 188.72] loss=1.45 avg=1.69\n",
            "[74 | 191.18] loss=1.38 avg=1.68\n",
            "[75 | 193.63] loss=1.43 avg=1.68\n",
            "Saving checkpoint/run1/model-75\n",
            "[76 | 198.39] loss=1.45 avg=1.67\n",
            "[77 | 200.83] loss=1.35 avg=1.66\n",
            "[78 | 203.29] loss=1.36 avg=1.66\n",
            "[79 | 205.74] loss=1.27 avg=1.65\n",
            "[80 | 208.19] loss=1.31 avg=1.65\n",
            "[81 | 210.64] loss=1.29 avg=1.64\n",
            "[82 | 213.09] loss=1.37 avg=1.63\n",
            "[83 | 215.54] loss=1.36 avg=1.63\n",
            "[84 | 217.99] loss=1.28 avg=1.62\n",
            "[85 | 220.44] loss=1.26 avg=1.62\n",
            "[86 | 222.89] loss=1.32 avg=1.61\n",
            "[87 | 225.34] loss=1.23 avg=1.61\n",
            "[88 | 227.79] loss=1.30 avg=1.60\n",
            "[89 | 230.24] loss=1.29 avg=1.60\n",
            "[90 | 232.69] loss=1.25 avg=1.59\n",
            "Saving checkpoint/run1/model-90\n",
            "[91 | 237.51] loss=1.31 avg=1.58\n",
            "[92 | 239.94] loss=1.31 avg=1.58\n",
            "[93 | 242.39] loss=1.33 avg=1.58\n",
            "[94 | 244.83] loss=1.31 avg=1.57\n",
            "[95 | 247.28] loss=1.18 avg=1.57\n",
            "[96 | 249.72] loss=1.30 avg=1.56\n",
            "[97 | 252.18] loss=1.20 avg=1.56\n",
            "[98 | 254.63] loss=1.27 avg=1.55\n",
            "[99 | 257.08] loss=1.30 avg=1.55\n",
            "[100 | 259.53] loss=1.20 avg=1.54\n",
            "======== SAMPLE 1 ========\n",
            " do a lot of different method to do so\n",
            " the previous one was a bit of a different cats.\n",
            " been able to get enough food for its own cats, and\n",
            " think it's a good cats to have a lot\n",
            " going to be able to get out of the cats\n",
            " in our community.\n",
            "\n",
            "\n",
            "\n",
            "The app is available for Android and iOS.\n",
            "\n",
            "\n",
            " think it's a good idea to have a cat of\n",
            " going to be able to do it.\n",
            "\n",
            "\n",
            "I'm not sure\n",
            "\n",
            "[101 | 264.11] loss=1.36 avg=1.54\n",
            "[102 | 266.57] loss=1.22 avg=1.53\n",
            "[103 | 269.02] loss=1.16 avg=1.53\n",
            "[104 | 271.47] loss=1.32 avg=1.52\n",
            "[105 | 273.92] loss=1.15 avg=1.52\n",
            "Saving checkpoint/run1/model-105\n",
            "[106 | 278.58] loss=1.34 avg=1.52\n",
            "[107 | 281.02] loss=1.23 avg=1.51\n",
            "[108 | 283.47] loss=1.15 avg=1.51\n",
            "[109 | 285.92] loss=1.23 avg=1.50\n",
            "[110 | 288.38] loss=1.20 avg=1.50\n",
            "[111 | 290.83] loss=1.20 avg=1.49\n",
            "[112 | 293.30] loss=1.26 avg=1.49\n",
            "[113 | 295.75] loss=1.19 avg=1.49\n",
            "[114 | 298.20] loss=1.24 avg=1.48\n",
            "[115 | 300.65] loss=1.17 avg=1.48\n",
            "[116 | 303.09] loss=1.18 avg=1.47\n",
            "[117 | 305.54] loss=1.24 avg=1.47\n",
            "[118 | 307.99] loss=1.28 avg=1.47\n",
            "[119 | 310.44] loss=1.22 avg=1.46\n",
            "[120 | 312.90] loss=1.20 avg=1.46\n",
            "Saving checkpoint/run1/model-120\n",
            "[121 | 317.65] loss=1.14 avg=1.45\n",
            "[122 | 320.09] loss=1.22 avg=1.45\n",
            "[123 | 322.54] loss=1.21 avg=1.45\n",
            "[124 | 324.98] loss=1.28 avg=1.45\n",
            "[125 | 327.41] loss=1.28 avg=1.44\n",
            "[126 | 329.85] loss=1.17 avg=1.44\n",
            "[127 | 332.31] loss=1.21 avg=1.44\n",
            "[128 | 334.77] loss=1.17 avg=1.43\n",
            "[129 | 337.22] loss=1.19 avg=1.43\n",
            "[130 | 339.67] loss=1.20 avg=1.43\n",
            "[131 | 342.12] loss=1.19 avg=1.42\n",
            "[132 | 344.57] loss=1.21 avg=1.42\n",
            "[133 | 347.02] loss=1.29 avg=1.42\n",
            "[134 | 349.47] loss=1.24 avg=1.42\n",
            "[135 | 351.93] loss=1.21 avg=1.41\n",
            "Saving checkpoint/run1/model-135\n",
            "[136 | 356.63] loss=1.24 avg=1.41\n",
            "[137 | 359.07] loss=1.23 avg=1.41\n",
            "[138 | 361.52] loss=1.24 avg=1.41\n",
            "[139 | 363.98] loss=1.24 avg=1.40\n",
            "[140 | 366.43] loss=1.19 avg=1.40\n",
            "[141 | 368.87] loss=1.23 avg=1.40\n",
            "[142 | 371.33] loss=1.09 avg=1.39\n",
            "[143 | 373.78] loss=1.16 avg=1.39\n",
            "[144 | 376.23] loss=1.15 avg=1.39\n",
            "[145 | 378.68] loss=1.23 avg=1.39\n",
            "[146 | 381.13] loss=1.17 avg=1.38\n",
            "[147 | 383.58] loss=1.11 avg=1.38\n",
            "[148 | 386.03] loss=1.23 avg=1.38\n",
            "[149 | 388.48] loss=1.15 avg=1.38\n",
            "[150 | 390.93] loss=1.24 avg=1.37\n",
            "Saving checkpoint/run1/model-150\n",
            "[151 | 395.73] loss=1.20 avg=1.37\n",
            "[152 | 398.18] loss=1.25 avg=1.37\n",
            "[153 | 400.63] loss=1.13 avg=1.37\n",
            "[154 | 403.09] loss=1.17 avg=1.36\n",
            "[155 | 405.54] loss=1.19 avg=1.36\n",
            "[156 | 407.99] loss=1.16 avg=1.36\n",
            "[157 | 410.44] loss=1.19 avg=1.36\n",
            "[158 | 412.91] loss=1.15 avg=1.35\n",
            "[159 | 415.36] loss=1.15 avg=1.35\n",
            "[160 | 417.81] loss=1.12 avg=1.35\n",
            "[161 | 420.26] loss=1.11 avg=1.35\n",
            "[162 | 422.71] loss=1.10 avg=1.34\n",
            "[163 | 425.16] loss=1.12 avg=1.34\n",
            "[164 | 427.61] loss=1.17 avg=1.34\n",
            "[165 | 430.06] loss=1.19 avg=1.34\n",
            "Saving checkpoint/run1/model-165\n",
            "[166 | 434.82] loss=1.19 avg=1.33\n",
            "[167 | 437.26] loss=1.17 avg=1.33\n",
            "[168 | 439.72] loss=1.16 avg=1.33\n",
            "[169 | 442.17] loss=1.11 avg=1.33\n",
            "[170 | 444.62] loss=1.21 avg=1.33\n",
            "[171 | 447.07] loss=1.21 avg=1.33\n",
            "[172 | 449.52] loss=1.07 avg=1.32\n",
            "[173 | 451.97] loss=1.16 avg=1.32\n",
            "[174 | 454.42] loss=1.17 avg=1.32\n",
            "[175 | 456.87] loss=1.11 avg=1.32\n",
            "[176 | 459.32] loss=1.10 avg=1.31\n",
            "[177 | 461.77] loss=1.09 avg=1.31\n",
            "[178 | 464.22] loss=1.22 avg=1.31\n",
            "[179 | 466.67] loss=1.11 avg=1.31\n",
            "[180 | 469.12] loss=1.13 avg=1.30\n",
            "Saving checkpoint/run1/model-180\n",
            "[181 | 473.86] loss=1.07 avg=1.30\n",
            "[182 | 476.30] loss=1.18 avg=1.30\n",
            "[183 | 478.75] loss=1.15 avg=1.30\n",
            "[184 | 481.21] loss=1.16 avg=1.30\n",
            "[185 | 483.66] loss=1.13 avg=1.30\n",
            "[186 | 486.11] loss=1.02 avg=1.29\n",
            "[187 | 488.56] loss=1.04 avg=1.29\n",
            "[188 | 491.01] loss=1.14 avg=1.29\n",
            "[189 | 493.46] loss=1.07 avg=1.28\n",
            "[190 | 495.91] loss=1.18 avg=1.28\n",
            "[191 | 498.36] loss=1.11 avg=1.28\n",
            "[192 | 500.81] loss=1.23 avg=1.28\n",
            "[193 | 503.28] loss=1.15 avg=1.28\n",
            "[194 | 505.75] loss=1.14 avg=1.28\n",
            "[195 | 508.22] loss=1.09 avg=1.28\n",
            "Saving checkpoint/run1/model-195\n",
            "[196 | 512.88] loss=1.19 avg=1.27\n",
            "[197 | 515.33] loss=1.12 avg=1.27\n",
            "[198 | 517.77] loss=1.07 avg=1.27\n",
            "[199 | 520.23] loss=1.09 avg=1.27\n",
            "[200 | 522.68] loss=1.11 avg=1.27\n",
            "======== SAMPLE 1 ========\n",
            " first two years, and the first cat I saw\n",
            "\"I'm not going to say that I'm\n",
            " not sure if it's a good idea to cats\n",
            " and the other five is the most popular and the\n",
            " a man of great character, and he was a\n",
            " the first thing I noticed was that the first thing\n",
            " the LGBT community.\n",
            "\n",
            "\"We are not\n",
            " that the first thing I noticed was that the first\n",
            " seven seven seven seven seven seven seven seven seven\n",
            " for.\n",
            "\n",
            "\n",
            "[201 | 526.07] loss=1.10 avg=1.26\n",
            "[202 | 528.53] loss=1.11 avg=1.26\n",
            "[203 | 531.00] loss=1.25 avg=1.26\n",
            "[204 | 533.46] loss=1.01 avg=1.26\n",
            "[205 | 535.91] loss=1.16 avg=1.26\n",
            "[206 | 538.38] loss=1.16 avg=1.26\n",
            "[207 | 540.83] loss=1.10 avg=1.26\n",
            "[208 | 543.28] loss=1.12 avg=1.25\n",
            "[209 | 545.75] loss=1.17 avg=1.25\n",
            "[210 | 548.20] loss=1.02 avg=1.25\n",
            "Saving checkpoint/run1/model-210\n",
            "[211 | 552.90] loss=1.17 avg=1.25\n",
            "[212 | 555.35] loss=1.20 avg=1.25\n",
            "[213 | 557.80] loss=1.15 avg=1.25\n",
            "[214 | 560.25] loss=1.18 avg=1.25\n",
            "[215 | 562.70] loss=1.13 avg=1.25\n",
            "[216 | 565.15] loss=1.11 avg=1.24\n",
            "[217 | 567.60] loss=1.06 avg=1.24\n",
            "[218 | 570.05] loss=1.08 avg=1.24\n",
            "[219 | 572.50] loss=1.11 avg=1.24\n",
            "[220 | 574.96] loss=1.19 avg=1.24\n",
            "[221 | 577.41] loss=1.08 avg=1.24\n",
            "[222 | 579.86] loss=1.14 avg=1.24\n",
            "[223 | 582.31] loss=1.15 avg=1.23\n",
            "[224 | 584.76] loss=1.10 avg=1.23\n",
            "[225 | 587.21] loss=1.13 avg=1.23\n",
            "Saving checkpoint/run1/model-225\n",
            "[226 | 591.99] loss=1.01 avg=1.23\n",
            "[227 | 594.43] loss=1.10 avg=1.23\n",
            "[228 | 596.87] loss=1.16 avg=1.23\n",
            "[229 | 599.32] loss=1.11 avg=1.23\n",
            "[230 | 601.77] loss=1.09 avg=1.22\n",
            "[231 | 604.22] loss=1.13 avg=1.22\n",
            "[232 | 606.68] loss=1.08 avg=1.22\n",
            "[233 | 609.13] loss=1.16 avg=1.22\n",
            "[234 | 611.58] loss=1.01 avg=1.22\n",
            "[235 | 614.04] loss=1.14 avg=1.22\n",
            "[236 | 616.49] loss=1.11 avg=1.22\n",
            "[237 | 618.96] loss=1.06 avg=1.22\n",
            "[238 | 621.41] loss=1.20 avg=1.21\n",
            "[239 | 623.86] loss=1.16 avg=1.21\n",
            "[240 | 626.31] loss=1.07 avg=1.21\n",
            "Saving checkpoint/run1/model-240\n",
            "[241 | 631.09] loss=1.05 avg=1.21\n",
            "[242 | 633.54] loss=1.15 avg=1.21\n",
            "[243 | 635.99] loss=1.12 avg=1.21\n",
            "[244 | 638.44] loss=1.08 avg=1.21\n",
            "[245 | 640.89] loss=1.04 avg=1.21\n",
            "[246 | 643.34] loss=1.11 avg=1.20\n",
            "[247 | 645.79] loss=1.08 avg=1.20\n",
            "[248 | 648.24] loss=1.07 avg=1.20\n",
            "[249 | 650.69] loss=1.10 avg=1.20\n",
            "[250 | 653.14] loss=1.05 avg=1.20\n",
            "[251 | 655.59] loss=1.10 avg=1.20\n",
            "[252 | 658.03] loss=1.12 avg=1.20\n",
            "[253 | 660.49] loss=1.15 avg=1.20\n",
            "[254 | 662.94] loss=1.14 avg=1.20\n",
            "[255 | 665.39] loss=1.03 avg=1.19\n",
            "Saving checkpoint/run1/model-255\n",
            "[256 | 670.20] loss=0.96 avg=1.19\n",
            "[257 | 672.64] loss=1.12 avg=1.19\n",
            "[258 | 675.09] loss=0.98 avg=1.19\n",
            "[259 | 677.54] loss=1.11 avg=1.19\n",
            "[260 | 679.99] loss=1.03 avg=1.19\n",
            "[261 | 682.45] loss=1.13 avg=1.19\n",
            "[262 | 684.90] loss=1.06 avg=1.18\n",
            "[263 | 687.34] loss=1.13 avg=1.18\n",
            "[264 | 689.79] loss=1.11 avg=1.18\n",
            "[265 | 692.25] loss=1.03 avg=1.18\n",
            "[266 | 694.70] loss=1.02 avg=1.18\n",
            "[267 | 697.15] loss=1.04 avg=1.18\n",
            "[268 | 699.60] loss=1.05 avg=1.18\n",
            "[269 | 702.05] loss=1.06 avg=1.18\n",
            "[270 | 704.50] loss=1.05 avg=1.17\n",
            "Saving checkpoint/run1/model-270\n",
            "[271 | 709.34] loss=1.04 avg=1.17\n",
            "[272 | 711.79] loss=1.10 avg=1.17\n",
            "[273 | 714.24] loss=1.09 avg=1.17\n",
            "[274 | 716.69] loss=1.04 avg=1.17\n",
            "[275 | 719.14] loss=1.06 avg=1.17\n",
            "[276 | 721.59] loss=1.08 avg=1.17\n",
            "[277 | 724.05] loss=1.13 avg=1.17\n",
            "[278 | 726.50] loss=1.08 avg=1.17\n",
            "[279 | 728.95] loss=1.10 avg=1.17\n",
            "[280 | 731.40] loss=1.13 avg=1.17\n",
            "[281 | 733.85] loss=1.05 avg=1.16\n",
            "[282 | 736.30] loss=1.05 avg=1.16\n",
            "[283 | 738.76] loss=0.96 avg=1.16\n",
            "[284 | 741.21] loss=1.09 avg=1.16\n",
            "[285 | 743.66] loss=1.04 avg=1.16\n",
            "Saving checkpoint/run1/model-285\n",
            "[286 | 748.42] loss=1.07 avg=1.16\n",
            "[287 | 750.87] loss=1.01 avg=1.16\n",
            "[288 | 753.32] loss=1.05 avg=1.15\n",
            "[289 | 755.77] loss=1.04 avg=1.15\n",
            "[290 | 758.22] loss=1.04 avg=1.15\n",
            "[291 | 760.67] loss=1.08 avg=1.15\n",
            "[292 | 763.13] loss=0.97 avg=1.15\n",
            "[293 | 765.57] loss=1.13 avg=1.15\n",
            "[294 | 768.03] loss=0.98 avg=1.15\n",
            "[295 | 770.48] loss=1.06 avg=1.15\n",
            "[296 | 772.92] loss=1.00 avg=1.15\n",
            "[297 | 775.38] loss=1.12 avg=1.14\n",
            "[298 | 777.83] loss=1.01 avg=1.14\n",
            "[299 | 780.28] loss=0.99 avg=1.14\n",
            "[300 | 782.73] loss=1.02 avg=1.14\n",
            "Saving checkpoint/run1/model-300\n",
            "======== SAMPLE 1 ========\n",
            " a lot of people who are not going to be able to\n",
            " I was so happy to see them.\n",
            "\n",
            "\n",
            " to get the right balance between the two.\n",
            " going to be able to get out of the cats\n",
            "I think it's a good thing that we're\n",
            "\n",
            "\n",
            "The first cats you need to do is\n",
            "\n",
            "\n",
            "The first two numbers are the cat of\n",
            " was that the first thing I noticed was that the\n",
            "\n",
            "\n",
            "The cat has announced it will be taking\n",
            "\n",
            "The first step is\n",
            "\n",
            "[301 | 788.51] loss=1.06 avg=1.14\n",
            "[302 | 790.96] loss=1.01 avg=1.14\n",
            "[303 | 793.41] loss=1.08 avg=1.14\n",
            "[304 | 795.86] loss=1.02 avg=1.14\n",
            "[305 | 798.31] loss=1.07 avg=1.14\n",
            "[306 | 800.76] loss=1.07 avg=1.14\n",
            "[307 | 803.21] loss=1.06 avg=1.13\n",
            "[308 | 805.67] loss=1.05 avg=1.13\n",
            "[309 | 808.12] loss=1.08 avg=1.13\n",
            "[310 | 810.57] loss=1.05 avg=1.13\n",
            "[311 | 813.03] loss=1.07 avg=1.13\n",
            "[312 | 815.50] loss=1.02 avg=1.13\n",
            "[313 | 817.97] loss=1.08 avg=1.13\n",
            "[314 | 820.41] loss=1.05 avg=1.13\n",
            "[315 | 822.87] loss=1.01 avg=1.13\n",
            "Saving checkpoint/run1/model-315\n",
            "[316 | 827.64] loss=1.05 avg=1.13\n",
            "[317 | 830.09] loss=1.04 avg=1.13\n",
            "[318 | 832.54] loss=1.05 avg=1.13\n",
            "[319 | 834.99] loss=1.05 avg=1.12\n",
            "[320 | 837.43] loss=1.01 avg=1.12\n",
            "[321 | 839.89] loss=1.04 avg=1.12\n",
            "[322 | 842.34] loss=1.04 avg=1.12\n",
            "[323 | 844.79] loss=1.06 avg=1.12\n",
            "[324 | 847.23] loss=1.04 avg=1.12\n",
            "[325 | 849.69] loss=1.07 avg=1.12\n",
            "[326 | 852.13] loss=0.98 avg=1.12\n",
            "[327 | 854.59] loss=1.06 avg=1.12\n",
            "[328 | 857.04] loss=1.03 avg=1.12\n",
            "[329 | 859.49] loss=1.01 avg=1.12\n",
            "[330 | 861.94] loss=0.93 avg=1.11\n",
            "Saving checkpoint/run1/model-330\n",
            "[331 | 866.80] loss=1.04 avg=1.11\n",
            "[332 | 869.25] loss=1.04 avg=1.11\n",
            "[333 | 871.70] loss=1.00 avg=1.11\n",
            "[334 | 874.16] loss=1.03 avg=1.11\n",
            "[335 | 876.61] loss=1.02 avg=1.11\n",
            "[336 | 879.06] loss=0.99 avg=1.11\n",
            "[337 | 881.51] loss=1.07 avg=1.11\n",
            "[338 | 883.96] loss=1.04 avg=1.11\n",
            "[339 | 886.43] loss=0.98 avg=1.11\n",
            "[340 | 888.89] loss=1.02 avg=1.10\n",
            "[341 | 891.36] loss=1.03 avg=1.10\n",
            "[342 | 893.83] loss=0.99 avg=1.10\n",
            "[343 | 896.30] loss=1.15 avg=1.10\n",
            "[344 | 898.77] loss=1.03 avg=1.10\n",
            "[345 | 901.24] loss=0.98 avg=1.10\n",
            "Saving checkpoint/run1/model-345\n",
            "[346 | 905.92] loss=1.03 avg=1.10\n",
            "[347 | 908.36] loss=1.07 avg=1.10\n",
            "[348 | 910.81] loss=1.03 avg=1.10\n",
            "[349 | 913.26] loss=1.09 avg=1.10\n",
            "[350 | 915.71] loss=0.99 avg=1.10\n",
            "[351 | 918.16] loss=1.05 avg=1.10\n",
            "[352 | 920.61] loss=1.03 avg=1.10\n",
            "[353 | 923.06] loss=1.04 avg=1.10\n",
            "[354 | 925.51] loss=1.00 avg=1.09\n",
            "[355 | 927.96] loss=1.03 avg=1.09\n",
            "[356 | 930.42] loss=1.02 avg=1.09\n",
            "[357 | 932.86] loss=1.03 avg=1.09\n",
            "[358 | 935.32] loss=1.01 avg=1.09\n",
            "[359 | 937.77] loss=1.00 avg=1.09\n",
            "[360 | 940.22] loss=1.01 avg=1.09\n",
            "Saving checkpoint/run1/model-360\n",
            "[361 | 945.02] loss=1.02 avg=1.09\n",
            "[362 | 947.45] loss=0.99 avg=1.09\n",
            "[363 | 949.90] loss=1.08 avg=1.09\n",
            "[364 | 952.36] loss=1.03 avg=1.09\n",
            "[365 | 954.80] loss=1.01 avg=1.09\n",
            "[366 | 957.25] loss=1.08 avg=1.09\n",
            "[367 | 959.70] loss=1.01 avg=1.09\n",
            "[368 | 962.15] loss=1.06 avg=1.09\n",
            "[369 | 964.61] loss=1.04 avg=1.09\n",
            "[370 | 967.08] loss=0.99 avg=1.08\n",
            "[371 | 969.54] loss=1.04 avg=1.08\n",
            "[372 | 972.02] loss=1.05 avg=1.08\n",
            "[373 | 974.49] loss=1.06 avg=1.08\n",
            "[374 | 976.96] loss=1.10 avg=1.08\n",
            "[375 | 979.40] loss=1.03 avg=1.08\n",
            "Saving checkpoint/run1/model-375\n",
            "[376 | 984.29] loss=0.97 avg=1.08\n",
            "[377 | 986.74] loss=1.01 avg=1.08\n",
            "[378 | 989.18] loss=0.98 avg=1.08\n",
            "[379 | 991.64] loss=0.97 avg=1.08\n",
            "[380 | 994.09] loss=1.01 avg=1.08\n",
            "[381 | 996.55] loss=1.00 avg=1.08\n",
            "[382 | 999.01] loss=1.06 avg=1.08\n",
            "[383 | 1001.48] loss=1.04 avg=1.08\n",
            "[384 | 1003.95] loss=1.02 avg=1.08\n",
            "[385 | 1006.42] loss=1.07 avg=1.08\n",
            "[386 | 1008.89] loss=1.00 avg=1.08\n",
            "[387 | 1011.36] loss=1.05 avg=1.08\n",
            "[388 | 1013.83] loss=1.02 avg=1.07\n",
            "[389 | 1016.29] loss=1.00 avg=1.07\n",
            "[390 | 1018.76] loss=1.03 avg=1.07\n",
            "Saving checkpoint/run1/model-390\n",
            "[391 | 1023.66] loss=1.00 avg=1.07\n",
            "[392 | 1026.10] loss=0.99 avg=1.07\n",
            "[393 | 1028.55] loss=1.00 avg=1.07\n",
            "[394 | 1031.00] loss=0.98 avg=1.07\n",
            "[395 | 1033.45] loss=0.98 avg=1.07\n",
            "[396 | 1035.90] loss=1.07 avg=1.07\n",
            "[397 | 1038.35] loss=1.02 avg=1.07\n",
            "[398 | 1040.80] loss=1.02 avg=1.07\n",
            "[399 | 1043.25] loss=1.00 avg=1.07\n",
            "[400 | 1045.70] loss=1.04 avg=1.07\n",
            "======== SAMPLE 1 ========\n",
            " is going on.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            ", is a major attraction for kayak and canoe.\n",
            "\n",
            "\n",
            "The first step is to create a new cat\n",
            " going to do this.' \"\n",
            "\n",
            "The dog is a cats of\n",
            " to say that I'm going to say that I\n",
            "\n",
            "\n",
            "The following table lists the number of cats\n",
            " be donating its own $100 million to the cat\n",
            ", Berkeley (1939)\n",
            "\n",
            "The Berkeley\n",
            " that the only way to get a good cat is\n",
            " I\n",
            "\n",
            "[401 | 1049.12] loss=0.98 avg=1.07\n",
            "[402 | 1051.56] loss=1.04 avg=1.07\n",
            "[403 | 1054.02] loss=1.05 avg=1.07\n",
            "[404 | 1056.47] loss=1.05 avg=1.07\n",
            "[405 | 1058.92] loss=1.05 avg=1.07\n",
            "Saving checkpoint/run1/model-405\n",
            "[406 | 1063.76] loss=1.04 avg=1.07\n",
            "[407 | 1066.21] loss=1.07 avg=1.07\n",
            "[408 | 1068.65] loss=1.00 avg=1.07\n",
            "[409 | 1071.12] loss=1.07 avg=1.07\n",
            "[410 | 1073.57] loss=1.02 avg=1.06\n",
            "[411 | 1076.02] loss=1.10 avg=1.07\n",
            "[412 | 1078.47] loss=0.95 avg=1.06\n",
            "[413 | 1080.92] loss=0.99 avg=1.06\n",
            "[414 | 1083.38] loss=1.02 avg=1.06\n",
            "[415 | 1085.83] loss=1.04 avg=1.06\n",
            "[416 | 1088.30] loss=0.96 avg=1.06\n",
            "[417 | 1090.75] loss=1.09 avg=1.06\n",
            "[418 | 1093.20] loss=1.01 avg=1.06\n",
            "[419 | 1095.66] loss=1.04 avg=1.06\n",
            "[420 | 1098.12] loss=1.01 avg=1.06\n",
            "Saving checkpoint/run1/model-420\n",
            "[421 | 1102.90] loss=0.92 avg=1.06\n",
            "[422 | 1105.35] loss=0.97 avg=1.06\n",
            "[423 | 1107.81] loss=1.11 avg=1.06\n",
            "[424 | 1110.25] loss=0.96 avg=1.06\n",
            "[425 | 1112.71] loss=0.96 avg=1.06\n",
            "[426 | 1115.16] loss=1.04 avg=1.06\n",
            "[427 | 1117.61] loss=1.06 avg=1.06\n",
            "[428 | 1120.06] loss=1.02 avg=1.06\n",
            "[429 | 1122.51] loss=1.02 avg=1.06\n",
            "[430 | 1124.96] loss=1.04 avg=1.06\n",
            "[431 | 1127.41] loss=1.01 avg=1.06\n",
            "[432 | 1129.86] loss=1.02 avg=1.05\n",
            "[433 | 1132.31] loss=1.02 avg=1.05\n",
            "[434 | 1134.76] loss=1.05 avg=1.05\n",
            "[435 | 1137.21] loss=1.01 avg=1.05\n",
            "Saving checkpoint/run1/model-435\n",
            "[436 | 1141.97] loss=0.96 avg=1.05\n",
            "[437 | 1144.42] loss=0.96 avg=1.05\n",
            "[438 | 1146.87] loss=0.98 avg=1.05\n",
            "[439 | 1149.32] loss=1.04 avg=1.05\n",
            "[440 | 1151.77] loss=1.04 avg=1.05\n",
            "[441 | 1154.22] loss=1.08 avg=1.05\n",
            "[442 | 1156.67] loss=1.03 avg=1.05\n",
            "[443 | 1159.12] loss=1.01 avg=1.05\n",
            "[444 | 1161.57] loss=1.01 avg=1.05\n",
            "[445 | 1164.02] loss=0.98 avg=1.05\n",
            "[446 | 1166.47] loss=0.98 avg=1.05\n",
            "[447 | 1168.93] loss=0.97 avg=1.05\n",
            "[448 | 1171.38] loss=1.03 avg=1.05\n",
            "[449 | 1173.83] loss=1.03 avg=1.05\n",
            "[450 | 1176.30] loss=1.04 avg=1.05\n",
            "Saving checkpoint/run1/model-450\n",
            "[451 | 1181.15] loss=0.97 avg=1.05\n",
            "[452 | 1183.58] loss=1.06 avg=1.05\n",
            "[453 | 1186.03] loss=1.00 avg=1.05\n",
            "[454 | 1188.48] loss=0.95 avg=1.05\n",
            "[455 | 1190.93] loss=0.96 avg=1.04\n",
            "[456 | 1193.38] loss=1.00 avg=1.04\n",
            "[457 | 1195.84] loss=0.96 avg=1.04\n",
            "[458 | 1198.29] loss=1.00 avg=1.04\n",
            "[459 | 1200.74] loss=1.04 avg=1.04\n",
            "[460 | 1203.19] loss=1.02 avg=1.04\n",
            "[461 | 1205.66] loss=1.00 avg=1.04\n",
            "[462 | 1208.11] loss=1.01 avg=1.04\n",
            "[463 | 1210.57] loss=0.93 avg=1.04\n",
            "[464 | 1213.02] loss=1.01 avg=1.04\n",
            "[465 | 1215.47] loss=1.03 avg=1.04\n",
            "Saving checkpoint/run1/model-465\n",
            "[466 | 1220.25] loss=0.97 avg=1.04\n",
            "[467 | 1222.70] loss=0.97 avg=1.04\n",
            "[468 | 1225.14] loss=0.96 avg=1.04\n",
            "[469 | 1227.60] loss=0.95 avg=1.04\n",
            "[470 | 1230.05] loss=0.98 avg=1.04\n",
            "[471 | 1232.50] loss=1.06 avg=1.04\n",
            "[472 | 1234.95] loss=1.04 avg=1.04\n",
            "[473 | 1237.40] loss=1.01 avg=1.04\n",
            "[474 | 1239.85] loss=0.98 avg=1.04\n",
            "[475 | 1242.30] loss=1.03 avg=1.04\n",
            "[476 | 1244.75] loss=0.94 avg=1.04\n",
            "[477 | 1247.20] loss=0.99 avg=1.03\n",
            "[478 | 1249.65] loss=0.94 avg=1.03\n",
            "[479 | 1252.10] loss=1.02 avg=1.03\n",
            "[480 | 1254.55] loss=0.99 avg=1.03\n",
            "Saving checkpoint/run1/model-480\n",
            "[481 | 1259.34] loss=0.96 avg=1.03\n",
            "[482 | 1261.78] loss=1.01 avg=1.03\n",
            "[483 | 1264.23] loss=0.97 avg=1.03\n",
            "[484 | 1266.68] loss=0.96 avg=1.03\n",
            "[485 | 1269.13] loss=0.89 avg=1.03\n",
            "[486 | 1271.59] loss=0.97 avg=1.03\n",
            "[487 | 1274.04] loss=1.01 avg=1.03\n",
            "[488 | 1276.49] loss=0.97 avg=1.03\n",
            "[489 | 1278.94] loss=1.01 avg=1.03\n",
            "[490 | 1281.40] loss=0.97 avg=1.03\n",
            "[491 | 1283.85] loss=0.99 avg=1.03\n",
            "[492 | 1286.30] loss=1.00 avg=1.03\n",
            "[493 | 1288.75] loss=1.01 avg=1.03\n",
            "[494 | 1291.20] loss=1.06 avg=1.03\n",
            "[495 | 1293.67] loss=0.94 avg=1.03\n",
            "Saving checkpoint/run1/model-495\n",
            "[496 | 1298.43] loss=0.96 avg=1.03\n",
            "[497 | 1300.88] loss=0.96 avg=1.02\n",
            "[498 | 1303.33] loss=1.00 avg=1.02\n",
            "[499 | 1305.78] loss=0.97 avg=1.02\n",
            "[500 | 1308.23] loss=0.94 avg=1.02\n",
            "======== SAMPLE 1 ========\n",
            "X is also a member of the cat of\n",
            " it's a good idea to have a cat of\n",
            " a good idea to have a lot of people\n",
            "-qY-qY-qY-\n",
            " was a little kid, but I was a\n",
            " of people who are not going to be able to\n",
            "\n",
            "The first step is to create a new cat\n",
            " are not afraid to go out and play.\n",
            " in the journal Science, found that the number\n",
            " who are the most important people in\n",
            "\n",
            "[501 | 1311.60] loss=1.05 avg=1.02\n",
            "[502 | 1314.06] loss=0.95 avg=1.02\n",
            "[503 | 1316.54] loss=1.01 avg=1.02\n",
            "[504 | 1319.00] loss=0.93 avg=1.02\n",
            "[505 | 1321.47] loss=0.95 avg=1.02\n",
            "[506 | 1323.92] loss=1.04 avg=1.02\n",
            "[507 | 1326.37] loss=1.08 avg=1.02\n",
            "[508 | 1328.82] loss=0.98 avg=1.02\n",
            "[509 | 1331.28] loss=0.98 avg=1.02\n",
            "[510 | 1333.75] loss=0.92 avg=1.02\n",
            "Saving checkpoint/run1/model-510\n",
            "[511 | 1338.47] loss=0.92 avg=1.02\n",
            "[512 | 1340.91] loss=0.94 avg=1.02\n",
            "[513 | 1343.37] loss=0.99 avg=1.02\n",
            "[514 | 1345.82] loss=0.98 avg=1.02\n",
            "[515 | 1348.26] loss=0.94 avg=1.02\n",
            "[516 | 1350.72] loss=1.05 avg=1.02\n",
            "[517 | 1353.17] loss=1.02 avg=1.02\n",
            "[518 | 1355.61] loss=0.94 avg=1.02\n",
            "[519 | 1358.07] loss=0.96 avg=1.02\n",
            "[520 | 1360.52] loss=0.98 avg=1.02\n",
            "[521 | 1362.97] loss=0.98 avg=1.01\n",
            "[522 | 1365.42] loss=0.91 avg=1.01\n",
            "[523 | 1367.87] loss=0.88 avg=1.01\n",
            "[524 | 1370.32] loss=1.02 avg=1.01\n",
            "[525 | 1372.77] loss=0.96 avg=1.01\n",
            "Saving checkpoint/run1/model-525\n",
            "[526 | 1377.61] loss=0.99 avg=1.01\n",
            "[527 | 1380.06] loss=0.96 avg=1.01\n",
            "[528 | 1382.51] loss=0.97 avg=1.01\n",
            "[529 | 1384.96] loss=0.92 avg=1.01\n",
            "[530 | 1387.41] loss=0.96 avg=1.01\n",
            "[531 | 1389.86] loss=1.04 avg=1.01\n",
            "[532 | 1392.31] loss=1.00 avg=1.01\n",
            "[533 | 1394.77] loss=0.98 avg=1.01\n",
            "[534 | 1397.21] loss=1.04 avg=1.01\n",
            "[535 | 1399.67] loss=0.96 avg=1.01\n",
            "[536 | 1402.11] loss=1.00 avg=1.01\n",
            "[537 | 1404.57] loss=0.99 avg=1.01\n",
            "[538 | 1407.01] loss=0.98 avg=1.01\n",
            "[539 | 1409.47] loss=0.98 avg=1.01\n",
            "[540 | 1411.94] loss=1.06 avg=1.01\n",
            "Saving checkpoint/run1/model-540\n",
            "[541 | 1416.70] loss=0.95 avg=1.01\n",
            "[542 | 1419.15] loss=1.00 avg=1.01\n",
            "[543 | 1421.60] loss=0.99 avg=1.01\n",
            "[544 | 1424.08] loss=1.04 avg=1.01\n",
            "[545 | 1426.52] loss=0.94 avg=1.01\n",
            "[546 | 1428.98] loss=0.94 avg=1.01\n",
            "[547 | 1431.42] loss=0.98 avg=1.01\n",
            "[548 | 1433.87] loss=0.93 avg=1.01\n",
            "[549 | 1436.33] loss=0.94 avg=1.00\n",
            "[550 | 1438.80] loss=0.97 avg=1.00\n",
            "[551 | 1441.25] loss=0.98 avg=1.00\n",
            "[552 | 1443.70] loss=0.95 avg=1.00\n",
            "[553 | 1446.16] loss=0.97 avg=1.00\n",
            "[554 | 1448.61] loss=0.92 avg=1.00\n",
            "[555 | 1451.06] loss=0.93 avg=1.00\n",
            "Saving checkpoint/run1/model-555\n",
            "[556 | 1455.91] loss=1.01 avg=1.00\n",
            "[557 | 1458.35] loss=1.01 avg=1.00\n",
            "[558 | 1460.81] loss=1.01 avg=1.00\n",
            "[559 | 1463.26] loss=1.01 avg=1.00\n",
            "[560 | 1465.71] loss=0.96 avg=1.00\n",
            "[561 | 1468.16] loss=0.97 avg=1.00\n",
            "[562 | 1470.61] loss=0.95 avg=1.00\n",
            "[563 | 1473.06] loss=0.98 avg=1.00\n",
            "[564 | 1475.51] loss=0.96 avg=1.00\n",
            "[565 | 1477.97] loss=0.96 avg=1.00\n",
            "[566 | 1480.42] loss=0.94 avg=1.00\n",
            "[567 | 1482.87] loss=0.89 avg=1.00\n",
            "[568 | 1485.32] loss=0.92 avg=1.00\n",
            "[569 | 1487.77] loss=0.94 avg=1.00\n",
            "[570 | 1490.24] loss=0.97 avg=1.00\n",
            "Saving checkpoint/run1/model-570\n",
            "[571 | 1495.04] loss=0.91 avg=1.00\n",
            "[572 | 1497.48] loss=0.95 avg=1.00\n",
            "[573 | 1499.94] loss=0.98 avg=1.00\n",
            "[574 | 1502.39] loss=1.01 avg=1.00\n",
            "[575 | 1504.84] loss=0.88 avg=0.99\n",
            "[576 | 1507.29] loss=0.88 avg=0.99\n",
            "[577 | 1509.74] loss=0.90 avg=0.99\n",
            "[578 | 1512.19] loss=1.02 avg=0.99\n",
            "[579 | 1514.64] loss=0.98 avg=0.99\n",
            "[580 | 1517.09] loss=0.93 avg=0.99\n",
            "[581 | 1519.54] loss=0.93 avg=0.99\n",
            "[582 | 1521.99] loss=0.95 avg=0.99\n",
            "[583 | 1524.44] loss=0.92 avg=0.99\n",
            "[584 | 1526.89] loss=0.96 avg=0.99\n",
            "[585 | 1529.34] loss=1.01 avg=0.99\n",
            "Saving checkpoint/run1/model-585\n",
            "[586 | 1534.13] loss=0.96 avg=0.99\n",
            "[587 | 1536.57] loss=0.99 avg=0.99\n",
            "[588 | 1539.02] loss=0.89 avg=0.99\n",
            "[589 | 1541.48] loss=0.95 avg=0.99\n",
            "[590 | 1543.92] loss=1.01 avg=0.99\n",
            "[591 | 1546.37] loss=0.92 avg=0.99\n",
            "[592 | 1548.83] loss=0.97 avg=0.99\n",
            "[593 | 1551.28] loss=1.00 avg=0.99\n",
            "[594 | 1553.73] loss=0.97 avg=0.99\n",
            "[595 | 1556.19] loss=0.93 avg=0.99\n",
            "[596 | 1558.64] loss=0.90 avg=0.99\n",
            "[597 | 1561.08] loss=0.96 avg=0.99\n",
            "[598 | 1563.53] loss=0.98 avg=0.99\n",
            "[599 | 1565.99] loss=0.96 avg=0.99\n",
            "[600 | 1568.44] loss=0.96 avg=0.98\n",
            "Saving checkpoint/run1/model-600\n",
            "======== SAMPLE 1 ========\n",
            " what the hell is going on here\n",
            " the most common form of the genus, the genus\n",
            "\" name is not a cats that is used\n",
            " and the cat of the top 10 most popular\n",
            " going to be able to afford to buy a new\n",
            " the cat \" to get a better look at the\n",
            " the fact that the US cats has been using the\n",
            " the same name, is a cats who has been\n",
            "uit or not.\n",
            "\n",
            "The best cat to\n",
            " for.\n",
            "\n",
            "\"We're\n",
            "\n",
            "[601 | 1574.02] loss=0.96 avg=0.98\n",
            "[602 | 1576.45] loss=0.96 avg=0.98\n",
            "[603 | 1578.90] loss=0.93 avg=0.98\n",
            "[604 | 1581.35] loss=0.91 avg=0.98\n",
            "[605 | 1583.80] loss=0.95 avg=0.98\n",
            "[606 | 1586.25] loss=0.91 avg=0.98\n",
            "[607 | 1588.70] loss=0.92 avg=0.98\n",
            "[608 | 1591.15] loss=0.93 avg=0.98\n",
            "[609 | 1593.60] loss=0.87 avg=0.98\n",
            "[610 | 1596.05] loss=0.98 avg=0.98\n",
            "[611 | 1598.51] loss=0.97 avg=0.98\n",
            "[612 | 1600.96] loss=0.94 avg=0.98\n",
            "[613 | 1603.41] loss=0.94 avg=0.98\n",
            "[614 | 1605.86] loss=0.91 avg=0.98\n",
            "[615 | 1608.31] loss=0.94 avg=0.98\n",
            "Saving checkpoint/run1/model-615\n",
            "[616 | 1613.08] loss=0.94 avg=0.98\n",
            "[617 | 1615.51] loss=0.91 avg=0.98\n",
            "[618 | 1617.96] loss=0.98 avg=0.98\n",
            "[619 | 1620.41] loss=0.93 avg=0.98\n",
            "[620 | 1622.86] loss=0.98 avg=0.98\n",
            "[621 | 1625.30] loss=0.95 avg=0.98\n",
            "[622 | 1627.75] loss=1.05 avg=0.98\n",
            "[623 | 1630.20] loss=0.98 avg=0.98\n",
            "[624 | 1632.64] loss=0.96 avg=0.98\n",
            "[625 | 1635.08] loss=0.95 avg=0.98\n",
            "[626 | 1637.53] loss=0.98 avg=0.98\n",
            "[627 | 1639.98] loss=0.96 avg=0.98\n",
            "[628 | 1642.42] loss=0.90 avg=0.98\n",
            "[629 | 1644.88] loss=0.96 avg=0.98\n",
            "[630 | 1647.32] loss=0.89 avg=0.97\n",
            "Saving checkpoint/run1/model-630\n",
            "[631 | 1652.02] loss=0.89 avg=0.97\n",
            "[632 | 1654.44] loss=0.93 avg=0.97\n",
            "[633 | 1656.87] loss=0.91 avg=0.97\n",
            "[634 | 1659.31] loss=0.97 avg=0.97\n",
            "[635 | 1661.75] loss=0.94 avg=0.97\n",
            "[636 | 1664.20] loss=0.89 avg=0.97\n",
            "[637 | 1666.65] loss=0.92 avg=0.97\n",
            "[638 | 1669.11] loss=0.93 avg=0.97\n",
            "[639 | 1671.55] loss=0.87 avg=0.97\n",
            "[640 | 1674.00] loss=0.95 avg=0.97\n",
            "[641 | 1676.45] loss=0.93 avg=0.97\n",
            "[642 | 1678.90] loss=0.89 avg=0.97\n",
            "[643 | 1681.36] loss=0.93 avg=0.97\n",
            "[644 | 1683.81] loss=0.88 avg=0.97\n",
            "[645 | 1686.26] loss=0.97 avg=0.97\n",
            "Saving checkpoint/run1/model-645\n",
            "[646 | 1691.06] loss=0.88 avg=0.97\n",
            "[647 | 1693.51] loss=0.93 avg=0.97\n",
            "[648 | 1695.96] loss=0.89 avg=0.97\n",
            "[649 | 1698.42] loss=0.89 avg=0.96\n",
            "[650 | 1700.86] loss=1.00 avg=0.96\n",
            "[651 | 1703.31] loss=0.89 avg=0.96\n",
            "[652 | 1705.77] loss=0.89 avg=0.96\n",
            "[653 | 1708.21] loss=0.97 avg=0.96\n",
            "[654 | 1710.66] loss=0.92 avg=0.96\n",
            "[655 | 1713.11] loss=0.90 avg=0.96\n",
            "[656 | 1715.57] loss=0.96 avg=0.96\n",
            "[657 | 1718.02] loss=0.95 avg=0.96\n",
            "[658 | 1720.49] loss=0.97 avg=0.96\n",
            "[659 | 1722.93] loss=0.97 avg=0.96\n",
            "[660 | 1725.38] loss=0.96 avg=0.96\n",
            "Saving checkpoint/run1/model-660\n",
            "[661 | 1730.02] loss=1.00 avg=0.96\n",
            "[662 | 1732.47] loss=0.90 avg=0.96\n",
            "[663 | 1734.92] loss=0.93 avg=0.96\n",
            "[664 | 1737.37] loss=1.02 avg=0.96\n",
            "[665 | 1739.82] loss=0.96 avg=0.96\n",
            "[666 | 1742.27] loss=0.90 avg=0.96\n",
            "[667 | 1744.72] loss=0.98 avg=0.96\n",
            "[668 | 1747.17] loss=0.93 avg=0.96\n",
            "[669 | 1749.63] loss=0.96 avg=0.96\n",
            "[670 | 1752.08] loss=0.96 avg=0.96\n",
            "[671 | 1754.53] loss=0.92 avg=0.96\n",
            "[672 | 1756.98] loss=0.92 avg=0.96\n",
            "[673 | 1759.43] loss=1.00 avg=0.96\n",
            "[674 | 1761.88] loss=0.90 avg=0.96\n",
            "[675 | 1764.33] loss=0.95 avg=0.96\n",
            "Saving checkpoint/run1/model-675\n",
            "[676 | 1769.12] loss=0.89 avg=0.96\n",
            "[677 | 1771.56] loss=0.93 avg=0.96\n",
            "[678 | 1774.01] loss=0.95 avg=0.96\n",
            "[679 | 1776.47] loss=0.95 avg=0.96\n",
            "[680 | 1778.91] loss=0.94 avg=0.96\n",
            "[681 | 1781.37] loss=0.93 avg=0.96\n",
            "[682 | 1783.82] loss=0.98 avg=0.96\n",
            "[683 | 1786.26] loss=0.90 avg=0.96\n",
            "[684 | 1788.72] loss=0.97 avg=0.96\n",
            "[685 | 1791.17] loss=0.88 avg=0.96\n",
            "[686 | 1793.62] loss=0.91 avg=0.96\n",
            "[687 | 1796.07] loss=0.98 avg=0.96\n",
            "[688 | 1798.52] loss=0.98 avg=0.96\n",
            "[689 | 1800.97] loss=0.95 avg=0.96\n",
            "[690 | 1803.42] loss=0.94 avg=0.96\n",
            "Saving checkpoint/run1/model-690\n",
            "[691 | 1808.22] loss=0.95 avg=0.96\n",
            "[692 | 1810.67] loss=0.98 avg=0.96\n",
            "[693 | 1813.11] loss=0.98 avg=0.96\n",
            "[694 | 1815.55] loss=0.91 avg=0.96\n",
            "[695 | 1818.01] loss=0.96 avg=0.96\n",
            "[696 | 1820.46] loss=0.93 avg=0.96\n",
            "[697 | 1822.90] loss=0.86 avg=0.96\n",
            "[698 | 1825.34] loss=0.92 avg=0.96\n",
            "[699 | 1827.79] loss=0.98 avg=0.96\n",
            "[700 | 1830.23] loss=0.97 avg=0.96\n",
            "======== SAMPLE 1 ========\n",
            "'m going to say that\n",
            " recent generation of the American cats.\n",
            "\n",
            "The\n",
            "The United States of America, which is a major\n",
            ", and the book is available in paperback and Kindle\n",
            "\n",
            "\n",
            "The first cat I noticed was that the\n",
            ". He's been a great guy. He's\n",
            " going to say that I'm going to say that\n",
            "-one zero twenty-one zero twenty-\n",
            " called \"config.json\" and add the following\n",
            " that the researchers had found a link between the\n",
            "\n",
            "[701 | 1833.60] loss=0.90 avg=0.96\n",
            "[702 | 1836.06] loss=0.89 avg=0.95\n",
            "[703 | 1838.53] loss=0.92 avg=0.95\n",
            "[704 | 1840.97] loss=0.98 avg=0.95\n",
            "[705 | 1843.42] loss=0.97 avg=0.95\n",
            "Saving checkpoint/run1/model-705\n",
            "[706 | 1848.12] loss=0.93 avg=0.95\n",
            "[707 | 1850.56] loss=0.96 avg=0.95\n",
            "[708 | 1853.02] loss=0.93 avg=0.95\n",
            "[709 | 1855.47] loss=0.95 avg=0.95\n",
            "[710 | 1857.92] loss=0.92 avg=0.95\n",
            "[711 | 1860.37] loss=0.92 avg=0.95\n",
            "[712 | 1862.82] loss=0.92 avg=0.95\n",
            "[713 | 1865.27] loss=0.97 avg=0.95\n",
            "[714 | 1867.72] loss=0.97 avg=0.95\n",
            "[715 | 1870.17] loss=0.93 avg=0.95\n",
            "[716 | 1872.62] loss=0.95 avg=0.95\n",
            "[717 | 1875.07] loss=0.93 avg=0.95\n",
            "[718 | 1877.52] loss=0.95 avg=0.95\n",
            "[719 | 1879.97] loss=0.86 avg=0.95\n",
            "[720 | 1882.43] loss=0.82 avg=0.95\n",
            "Saving checkpoint/run1/model-720\n",
            "[721 | 1887.17] loss=0.94 avg=0.95\n",
            "[722 | 1889.61] loss=0.98 avg=0.95\n",
            "[723 | 1892.07] loss=0.90 avg=0.95\n",
            "[724 | 1894.52] loss=0.92 avg=0.95\n",
            "[725 | 1896.97] loss=0.95 avg=0.95\n",
            "[726 | 1899.42] loss=0.89 avg=0.95\n",
            "[727 | 1901.87] loss=0.91 avg=0.95\n",
            "[728 | 1904.32] loss=0.90 avg=0.95\n",
            "[729 | 1906.77] loss=0.97 avg=0.95\n",
            "[730 | 1909.22] loss=1.03 avg=0.95\n",
            "[731 | 1911.67] loss=0.96 avg=0.95\n",
            "[732 | 1914.12] loss=0.92 avg=0.95\n",
            "[733 | 1916.57] loss=0.91 avg=0.95\n",
            "[734 | 1919.02] loss=0.91 avg=0.95\n",
            "[735 | 1921.47] loss=0.92 avg=0.95\n",
            "Saving checkpoint/run1/model-735\n",
            "[736 | 1926.18] loss=0.95 avg=0.95\n",
            "[737 | 1928.61] loss=0.96 avg=0.95\n",
            "[738 | 1931.05] loss=0.96 avg=0.95\n",
            "[739 | 1933.51] loss=0.92 avg=0.95\n",
            "[740 | 1935.96] loss=0.91 avg=0.95\n",
            "[741 | 1938.41] loss=0.90 avg=0.95\n",
            "[742 | 1940.85] loss=0.93 avg=0.95\n",
            "[743 | 1943.30] loss=0.85 avg=0.95\n",
            "[744 | 1945.76] loss=0.99 avg=0.95\n",
            "[745 | 1948.21] loss=0.94 avg=0.95\n",
            "[746 | 1950.66] loss=0.92 avg=0.95\n",
            "[747 | 1953.11] loss=0.96 avg=0.95\n",
            "[748 | 1955.56] loss=0.88 avg=0.95\n",
            "[749 | 1958.01] loss=0.99 avg=0.95\n",
            "[750 | 1960.46] loss=0.84 avg=0.95\n",
            "Saving checkpoint/run1/model-750\n",
            "[751 | 1965.19] loss=0.92 avg=0.95\n",
            "[752 | 1967.62] loss=0.96 avg=0.95\n",
            "[753 | 1970.06] loss=0.96 avg=0.95\n",
            "[754 | 1972.52] loss=0.95 avg=0.95\n",
            "[755 | 1974.96] loss=0.99 avg=0.95\n",
            "[756 | 1977.42] loss=0.94 avg=0.95\n",
            "[757 | 1979.87] loss=0.92 avg=0.95\n",
            "[758 | 1982.32] loss=0.92 avg=0.95\n",
            "[759 | 1984.77] loss=0.86 avg=0.94\n",
            "[760 | 1987.22] loss=0.96 avg=0.94\n",
            "[761 | 1989.67] loss=0.90 avg=0.94\n",
            "[762 | 1992.12] loss=0.88 avg=0.94\n",
            "[763 | 1994.57] loss=0.92 avg=0.94\n",
            "[764 | 1997.02] loss=0.96 avg=0.94\n",
            "[765 | 1999.47] loss=0.88 avg=0.94\n",
            "Saving checkpoint/run1/model-765\n",
            "[766 | 2004.27] loss=0.91 avg=0.94\n",
            "[767 | 2006.72] loss=0.94 avg=0.94\n",
            "[768 | 2009.17] loss=0.94 avg=0.94\n",
            "[769 | 2011.63] loss=0.91 avg=0.94\n",
            "[770 | 2014.08] loss=0.98 avg=0.94\n",
            "[771 | 2016.52] loss=0.85 avg=0.94\n",
            "[772 | 2018.98] loss=0.86 avg=0.94\n",
            "[773 | 2021.43] loss=0.89 avg=0.94\n",
            "[774 | 2023.88] loss=0.93 avg=0.94\n",
            "[775 | 2026.33] loss=0.85 avg=0.94\n",
            "[776 | 2028.78] loss=0.89 avg=0.94\n",
            "[777 | 2031.23] loss=0.96 avg=0.94\n",
            "[778 | 2033.68] loss=0.94 avg=0.94\n",
            "[779 | 2036.14] loss=0.93 avg=0.94\n",
            "[780 | 2038.58] loss=0.90 avg=0.94\n",
            "Saving checkpoint/run1/model-780\n",
            "[781 | 2043.38] loss=0.89 avg=0.94\n",
            "[782 | 2045.81] loss=0.89 avg=0.94\n",
            "[783 | 2048.25] loss=0.88 avg=0.94\n",
            "[784 | 2050.70] loss=0.89 avg=0.94\n",
            "[785 | 2053.15] loss=0.92 avg=0.94\n",
            "[786 | 2055.60] loss=0.89 avg=0.94\n",
            "[787 | 2058.05] loss=0.88 avg=0.94\n",
            "[788 | 2060.50] loss=0.89 avg=0.93\n",
            "[789 | 2062.95] loss=0.92 avg=0.93\n",
            "[790 | 2065.40] loss=0.83 avg=0.93\n",
            "[791 | 2067.85] loss=0.88 avg=0.93\n",
            "[792 | 2070.30] loss=0.91 avg=0.93\n",
            "[793 | 2072.75] loss=0.87 avg=0.93\n",
            "[794 | 2075.20] loss=0.89 avg=0.93\n",
            "[795 | 2077.65] loss=0.89 avg=0.93\n",
            "Saving checkpoint/run1/model-795\n",
            "[796 | 2082.39] loss=0.98 avg=0.93\n",
            "[797 | 2084.84] loss=0.87 avg=0.93\n",
            "[798 | 2087.29] loss=0.93 avg=0.93\n",
            "[799 | 2089.75] loss=0.87 avg=0.93\n",
            "[800 | 2092.20] loss=0.86 avg=0.93\n",
            "======== SAMPLE 1 ========\n",
            "amba \"I am a cats\" and\n",
            " it's a good idea to use a different method\n",
            "\n",
            "\n",
            "The first cats to note is that the\n",
            "\n",
            "\n",
            "The first step is to create a new\n",
            " version of the latest version of the latest version of\n",
            ".\n",
            "\n",
            "The first cat is to create a\n",
            " going to say that I'm going to say that\n",
            " think it's a good idea to have a cat\n",
            " going to be able to afford to buy a new\n",
            "\" version of the latest version\n",
            "\n",
            "[801 | 2095.57] loss=0.91 avg=0.93\n",
            "[802 | 2098.03] loss=0.84 avg=0.93\n",
            "[803 | 2100.50] loss=0.93 avg=0.93\n",
            "[804 | 2102.95] loss=0.94 avg=0.93\n",
            "[805 | 2105.40] loss=0.86 avg=0.93\n",
            "[806 | 2107.85] loss=0.82 avg=0.93\n",
            "[807 | 2110.32] loss=0.88 avg=0.93\n",
            "[808 | 2112.77] loss=0.89 avg=0.93\n",
            "[809 | 2115.22] loss=0.90 avg=0.93\n",
            "[810 | 2117.66] loss=0.95 avg=0.93\n",
            "Saving checkpoint/run1/model-810\n",
            "[811 | 2122.35] loss=0.89 avg=0.93\n",
            "[812 | 2124.78] loss=0.85 avg=0.93\n",
            "[813 | 2127.24] loss=0.94 avg=0.93\n",
            "[814 | 2129.69] loss=0.93 avg=0.93\n",
            "[815 | 2132.14] loss=0.94 avg=0.93\n",
            "[816 | 2134.59] loss=0.87 avg=0.93\n",
            "[817 | 2137.04] loss=0.91 avg=0.92\n",
            "[818 | 2139.48] loss=0.95 avg=0.93\n",
            "[819 | 2141.93] loss=0.92 avg=0.93\n",
            "[820 | 2144.39] loss=0.89 avg=0.92\n",
            "[821 | 2146.83] loss=0.96 avg=0.93\n",
            "[822 | 2149.28] loss=0.85 avg=0.92\n",
            "[823 | 2151.73] loss=0.89 avg=0.92\n",
            "[824 | 2154.19] loss=0.87 avg=0.92\n",
            "[825 | 2156.64] loss=0.88 avg=0.92\n",
            "Saving checkpoint/run1/model-825\n",
            "[826 | 2161.42] loss=0.87 avg=0.92\n",
            "[827 | 2163.87] loss=0.91 avg=0.92\n",
            "[828 | 2166.32] loss=0.93 avg=0.92\n",
            "[829 | 2168.77] loss=0.85 avg=0.92\n",
            "[830 | 2171.22] loss=0.89 avg=0.92\n",
            "[831 | 2173.66] loss=0.89 avg=0.92\n",
            "[832 | 2176.12] loss=0.95 avg=0.92\n",
            "[833 | 2178.57] loss=0.93 avg=0.92\n",
            "[834 | 2181.02] loss=0.89 avg=0.92\n",
            "[835 | 2183.47] loss=0.94 avg=0.92\n",
            "[836 | 2185.93] loss=0.86 avg=0.92\n",
            "[837 | 2188.39] loss=0.91 avg=0.92\n",
            "[838 | 2190.84] loss=0.89 avg=0.92\n",
            "[839 | 2193.29] loss=0.88 avg=0.92\n",
            "[840 | 2195.74] loss=0.94 avg=0.92\n",
            "Saving checkpoint/run1/model-840\n",
            "[841 | 2200.53] loss=0.86 avg=0.92\n",
            "[842 | 2202.97] loss=0.81 avg=0.92\n",
            "[843 | 2205.42] loss=0.86 avg=0.92\n",
            "[844 | 2207.88] loss=0.87 avg=0.92\n",
            "[845 | 2210.33] loss=0.90 avg=0.92\n",
            "[846 | 2212.77] loss=0.88 avg=0.92\n",
            "[847 | 2215.22] loss=0.88 avg=0.92\n",
            "[848 | 2217.68] loss=0.88 avg=0.92\n",
            "[849 | 2220.13] loss=0.98 avg=0.92\n",
            "[850 | 2222.58] loss=0.83 avg=0.92\n",
            "[851 | 2225.03] loss=0.87 avg=0.92\n",
            "[852 | 2227.48] loss=0.84 avg=0.92\n",
            "[853 | 2229.93] loss=0.89 avg=0.91\n",
            "[854 | 2232.38] loss=0.92 avg=0.91\n",
            "[855 | 2234.83] loss=0.92 avg=0.91\n",
            "Saving checkpoint/run1/model-855\n",
            "[856 | 2239.58] loss=0.87 avg=0.91\n",
            "[857 | 2242.02] loss=0.88 avg=0.91\n",
            "[858 | 2244.47] loss=0.90 avg=0.91\n",
            "[859 | 2246.93] loss=0.92 avg=0.91\n",
            "[860 | 2249.37] loss=0.87 avg=0.91\n",
            "[861 | 2251.82] loss=0.97 avg=0.91\n",
            "[862 | 2254.28] loss=0.91 avg=0.91\n",
            "[863 | 2256.72] loss=0.85 avg=0.91\n",
            "[864 | 2259.17] loss=0.88 avg=0.91\n",
            "[865 | 2261.62] loss=0.88 avg=0.91\n",
            "[866 | 2264.07] loss=0.85 avg=0.91\n",
            "[867 | 2266.52] loss=0.90 avg=0.91\n",
            "[868 | 2268.97] loss=0.86 avg=0.91\n",
            "[869 | 2271.42] loss=0.86 avg=0.91\n",
            "[870 | 2273.87] loss=0.89 avg=0.91\n",
            "Saving checkpoint/run1/model-870\n",
            "[871 | 2278.62] loss=0.96 avg=0.91\n",
            "[872 | 2281.06] loss=0.93 avg=0.91\n",
            "[873 | 2283.52] loss=0.88 avg=0.91\n",
            "[874 | 2285.97] loss=0.92 avg=0.91\n",
            "[875 | 2288.41] loss=0.87 avg=0.91\n",
            "[876 | 2290.87] loss=0.84 avg=0.91\n",
            "[877 | 2293.32] loss=0.91 avg=0.91\n",
            "[878 | 2295.77] loss=0.92 avg=0.91\n",
            "[879 | 2298.22] loss=0.98 avg=0.91\n",
            "[880 | 2300.67] loss=0.92 avg=0.91\n",
            "[881 | 2303.11] loss=0.91 avg=0.91\n",
            "[882 | 2305.57] loss=0.85 avg=0.91\n",
            "[883 | 2308.02] loss=0.91 avg=0.91\n",
            "[884 | 2310.47] loss=0.89 avg=0.91\n",
            "[885 | 2312.92] loss=0.90 avg=0.91\n",
            "Saving checkpoint/run1/model-885\n",
            "[886 | 2317.56] loss=0.86 avg=0.91\n",
            "[887 | 2320.01] loss=0.91 avg=0.91\n",
            "[888 | 2322.46] loss=0.85 avg=0.91\n",
            "[889 | 2324.91] loss=0.90 avg=0.91\n",
            "[890 | 2327.36] loss=0.88 avg=0.91\n",
            "[891 | 2329.81] loss=0.88 avg=0.91\n",
            "[892 | 2332.26] loss=0.90 avg=0.91\n",
            "[893 | 2334.71] loss=0.84 avg=0.91\n",
            "[894 | 2337.16] loss=0.89 avg=0.91\n",
            "[895 | 2339.61] loss=0.85 avg=0.91\n",
            "[896 | 2342.06] loss=0.84 avg=0.91\n",
            "[897 | 2344.51] loss=0.84 avg=0.91\n",
            "[898 | 2346.96] loss=0.94 avg=0.91\n",
            "[899 | 2349.41] loss=0.85 avg=0.91\n",
            "[900 | 2351.86] loss=0.88 avg=0.91\n",
            "Saving checkpoint/run1/model-900\n",
            "======== SAMPLE 1 ========\n",
            " cat going on here\n",
            " think it's a good idea to have a cat\n",
            " are not the only cat who are excited about the\n",
            " I'm going to be able to do it.\n",
            " think it's a good idea to have a cat\n",
            " get a good deal on the new cats.\n",
            "\n",
            " to be killed\" campaign.\n",
            "\n",
            "The cat\n",
            "'s a good cats that we're not doing this.\n",
            " going to say that I'm going to say that\n",
            " a new file.\n",
            "\n",
            "The cat\n",
            "\n",
            "[901 | 2357.50] loss=0.88 avg=0.90\n",
            "[902 | 2359.94] loss=0.90 avg=0.90\n",
            "[903 | 2362.39] loss=0.88 avg=0.90\n",
            "[904 | 2364.84] loss=0.85 avg=0.90\n",
            "[905 | 2367.29] loss=0.96 avg=0.90\n",
            "[906 | 2369.74] loss=0.90 avg=0.90\n",
            "[907 | 2372.20] loss=0.92 avg=0.90\n",
            "[908 | 2374.64] loss=0.84 avg=0.90\n",
            "[909 | 2377.10] loss=0.90 avg=0.90\n",
            "[910 | 2379.54] loss=1.02 avg=0.91\n",
            "[911 | 2381.99] loss=0.84 avg=0.90\n",
            "[912 | 2384.44] loss=0.90 avg=0.90\n",
            "[913 | 2386.90] loss=0.86 avg=0.90\n",
            "[914 | 2389.34] loss=0.85 avg=0.90\n",
            "[915 | 2391.79] loss=0.91 avg=0.90\n",
            "Saving checkpoint/run1/model-915\n",
            "[916 | 2396.54] loss=0.89 avg=0.90\n",
            "[917 | 2398.98] loss=0.90 avg=0.90\n",
            "[918 | 2401.43] loss=0.90 avg=0.90\n",
            "[919 | 2403.88] loss=0.84 avg=0.90\n",
            "[920 | 2406.33] loss=0.92 avg=0.90\n",
            "[921 | 2408.78] loss=0.88 avg=0.90\n",
            "[922 | 2411.23] loss=0.91 avg=0.90\n",
            "[923 | 2413.68] loss=0.82 avg=0.90\n",
            "[924 | 2416.13] loss=0.94 avg=0.90\n",
            "[925 | 2418.58] loss=0.85 avg=0.90\n",
            "[926 | 2421.03] loss=0.87 avg=0.90\n",
            "[927 | 2423.49] loss=0.94 avg=0.90\n",
            "[928 | 2425.94] loss=0.88 avg=0.90\n",
            "[929 | 2428.39] loss=0.82 avg=0.90\n",
            "[930 | 2430.84] loss=0.85 avg=0.90\n",
            "Saving checkpoint/run1/model-930\n",
            "[931 | 2435.64] loss=0.90 avg=0.90\n",
            "[932 | 2438.09] loss=0.87 avg=0.90\n",
            "[933 | 2440.54] loss=0.87 avg=0.90\n",
            "[934 | 2442.99] loss=0.83 avg=0.90\n",
            "[935 | 2445.44] loss=0.94 avg=0.90\n",
            "[936 | 2447.89] loss=0.84 avg=0.90\n",
            "[937 | 2450.34] loss=0.88 avg=0.90\n",
            "[938 | 2452.79] loss=0.85 avg=0.90\n",
            "[939 | 2455.25] loss=0.90 avg=0.90\n",
            "[940 | 2457.70] loss=0.87 avg=0.90\n",
            "[941 | 2460.15] loss=0.93 avg=0.90\n",
            "[942 | 2462.59] loss=0.83 avg=0.90\n",
            "[943 | 2465.05] loss=0.86 avg=0.90\n",
            "[944 | 2467.50] loss=0.81 avg=0.90\n",
            "[945 | 2469.95] loss=0.83 avg=0.90\n",
            "Saving checkpoint/run1/model-945\n",
            "[946 | 2474.71] loss=0.83 avg=0.89\n",
            "[947 | 2477.15] loss=0.89 avg=0.89\n",
            "[948 | 2479.60] loss=0.85 avg=0.89\n",
            "[949 | 2482.06] loss=0.93 avg=0.89\n",
            "[950 | 2484.51] loss=0.87 avg=0.89\n",
            "[951 | 2486.95] loss=0.85 avg=0.89\n",
            "[952 | 2489.41] loss=0.85 avg=0.89\n",
            "[953 | 2491.85] loss=0.91 avg=0.89\n",
            "[954 | 2494.31] loss=0.85 avg=0.89\n",
            "[955 | 2496.76] loss=0.83 avg=0.89\n",
            "[956 | 2499.21] loss=0.87 avg=0.89\n",
            "[957 | 2501.67] loss=0.88 avg=0.89\n",
            "[958 | 2504.12] loss=0.88 avg=0.89\n",
            "[959 | 2506.57] loss=0.81 avg=0.89\n",
            "[960 | 2509.01] loss=0.84 avg=0.89\n",
            "Saving checkpoint/run1/model-960\n",
            "[961 | 2513.74] loss=0.92 avg=0.89\n",
            "[962 | 2516.18] loss=0.84 avg=0.89\n",
            "[963 | 2518.64] loss=0.90 avg=0.89\n",
            "[964 | 2521.09] loss=0.87 avg=0.89\n",
            "[965 | 2523.54] loss=0.85 avg=0.89\n",
            "[966 | 2525.99] loss=0.86 avg=0.89\n",
            "[967 | 2528.44] loss=0.84 avg=0.89\n",
            "[968 | 2530.91] loss=0.82 avg=0.89\n",
            "[969 | 2533.38] loss=0.86 avg=0.89\n",
            "[970 | 2535.85] loss=0.87 avg=0.89\n",
            "[971 | 2538.32] loss=0.87 avg=0.89\n",
            "[972 | 2540.77] loss=0.80 avg=0.89\n",
            "[973 | 2543.22] loss=0.84 avg=0.89\n",
            "[974 | 2545.67] loss=0.87 avg=0.89\n",
            "[975 | 2548.12] loss=0.87 avg=0.89\n",
            "Saving checkpoint/run1/model-975\n",
            "[976 | 2552.85] loss=0.88 avg=0.89\n",
            "[977 | 2555.28] loss=0.87 avg=0.89\n",
            "[978 | 2557.72] loss=0.92 avg=0.89\n",
            "[979 | 2560.17] loss=0.89 avg=0.89\n",
            "[980 | 2562.62] loss=0.82 avg=0.89\n",
            "[981 | 2565.06] loss=0.85 avg=0.89\n",
            "[982 | 2567.50] loss=0.86 avg=0.89\n",
            "[983 | 2569.93] loss=0.88 avg=0.89\n",
            "[984 | 2572.37] loss=0.90 avg=0.89\n",
            "[985 | 2574.81] loss=0.91 avg=0.89\n",
            "[986 | 2577.27] loss=0.85 avg=0.89\n",
            "[987 | 2579.72] loss=0.87 avg=0.89\n",
            "[988 | 2582.16] loss=0.88 avg=0.89\n",
            "[989 | 2584.61] loss=0.86 avg=0.88\n",
            "[990 | 2587.05] loss=0.84 avg=0.88\n",
            "Saving checkpoint/run1/model-990\n",
            "[991 | 2591.81] loss=0.89 avg=0.88\n",
            "[992 | 2594.24] loss=0.92 avg=0.88\n",
            "[993 | 2596.68] loss=0.89 avg=0.88\n",
            "[994 | 2599.12] loss=0.88 avg=0.88\n",
            "[995 | 2601.57] loss=0.83 avg=0.88\n",
            "[996 | 2604.02] loss=0.88 avg=0.88\n",
            "[997 | 2606.47] loss=0.83 avg=0.88\n",
            "[998 | 2608.92] loss=0.89 avg=0.88\n",
            "[999 | 2611.37] loss=0.90 avg=0.88\n",
            "[1000 | 2613.82] loss=0.87 avg=0.88\n",
            "======== SAMPLE 1 ========\n",
            " is\n",
            " be released in the coming cat.\n",
            "\n",
            "The\n",
            "-day course, the class is designed to\n",
            " a man of great courage and of great cats.\n",
            "\n",
            "\n",
            "The first cats I saw the film was\n",
            " going to say that I'm going to say that\n",
            "\n",
            "The first step is to create a new cats\n",
            " to say that I'm going to say that I\n",
            " been killed by the cat.\n",
            "\n",
            "The first\n",
            " get the right balance between the two.\n",
            "\n",
            "\n",
            " were found in\n",
            "\n",
            "[1001 | 2617.18] loss=0.90 avg=0.88\n",
            "[1002 | 2619.63] loss=0.83 avg=0.88\n",
            "[1003 | 2622.10] loss=0.83 avg=0.88\n",
            "[1004 | 2624.55] loss=0.84 avg=0.88\n",
            "[1005 | 2627.00] loss=0.87 avg=0.88\n",
            "Saving checkpoint/run1/model-1005\n",
            "[1006 | 2631.77] loss=0.90 avg=0.88\n",
            "[1007 | 2634.21] loss=0.84 avg=0.88\n",
            "[1008 | 2636.67] loss=0.82 avg=0.88\n",
            "[1009 | 2639.10] loss=0.81 avg=0.88\n",
            "[1010 | 2641.55] loss=0.87 avg=0.88\n",
            "[1011 | 2643.99] loss=0.88 avg=0.88\n",
            "[1012 | 2646.42] loss=0.89 avg=0.88\n",
            "[1013 | 2648.87] loss=0.89 avg=0.88\n",
            "[1014 | 2651.31] loss=0.96 avg=0.88\n",
            "[1015 | 2653.76] loss=0.84 avg=0.88\n",
            "[1016 | 2656.19] loss=0.84 avg=0.88\n",
            "[1017 | 2658.64] loss=0.91 avg=0.88\n",
            "[1018 | 2661.07] loss=0.87 avg=0.88\n",
            "[1019 | 2663.53] loss=0.92 avg=0.88\n",
            "[1020 | 2665.98] loss=0.87 avg=0.88\n",
            "Saving checkpoint/run1/model-1020\n",
            "[1021 | 2670.71] loss=0.89 avg=0.88\n",
            "[1022 | 2673.15] loss=0.85 avg=0.88\n",
            "[1023 | 2675.60] loss=0.84 avg=0.88\n",
            "[1024 | 2678.04] loss=0.80 avg=0.88\n",
            "[1025 | 2680.48] loss=0.83 avg=0.88\n",
            "[1026 | 2682.94] loss=0.85 avg=0.88\n",
            "[1027 | 2685.39] loss=0.89 avg=0.88\n",
            "[1028 | 2687.83] loss=0.89 avg=0.88\n",
            "[1029 | 2690.28] loss=0.85 avg=0.88\n",
            "[1030 | 2692.74] loss=0.88 avg=0.88\n",
            "[1031 | 2695.19] loss=0.84 avg=0.88\n",
            "[1032 | 2697.64] loss=0.83 avg=0.88\n",
            "[1033 | 2700.09] loss=0.88 avg=0.88\n",
            "[1034 | 2702.54] loss=0.87 avg=0.88\n",
            "[1035 | 2705.00] loss=0.88 avg=0.88\n",
            "Saving checkpoint/run1/model-1035\n",
            "[1036 | 2709.68] loss=0.85 avg=0.88\n",
            "[1037 | 2712.12] loss=0.88 avg=0.88\n",
            "[1038 | 2714.57] loss=0.93 avg=0.88\n",
            "[1039 | 2717.02] loss=0.86 avg=0.88\n",
            "[1040 | 2719.48] loss=0.83 avg=0.88\n",
            "[1041 | 2721.92] loss=0.88 avg=0.88\n",
            "[1042 | 2724.38] loss=0.87 avg=0.88\n",
            "[1043 | 2726.83] loss=0.86 avg=0.88\n",
            "[1044 | 2729.28] loss=0.85 avg=0.88\n",
            "[1045 | 2731.73] loss=0.87 avg=0.88\n",
            "[1046 | 2734.18] loss=0.82 avg=0.88\n",
            "[1047 | 2736.63] loss=0.86 avg=0.88\n",
            "[1048 | 2739.08] loss=0.89 avg=0.88\n",
            "[1049 | 2741.53] loss=0.78 avg=0.88\n",
            "[1050 | 2743.98] loss=0.89 avg=0.88\n",
            "Saving checkpoint/run1/model-1050\n",
            "[1051 | 2748.81] loss=0.93 avg=0.88\n",
            "[1052 | 2751.25] loss=0.83 avg=0.88\n",
            "[1053 | 2753.70] loss=0.95 avg=0.88\n",
            "[1054 | 2756.15] loss=0.89 avg=0.88\n",
            "[1055 | 2758.60] loss=0.80 avg=0.88\n",
            "[1056 | 2761.05] loss=0.86 avg=0.88\n",
            "[1057 | 2763.51] loss=0.85 avg=0.88\n",
            "[1058 | 2765.96] loss=0.90 avg=0.88\n",
            "[1059 | 2768.41] loss=0.83 avg=0.88\n",
            "[1060 | 2770.86] loss=0.88 avg=0.88\n",
            "[1061 | 2773.32] loss=0.80 avg=0.87\n",
            "[1062 | 2775.77] loss=0.86 avg=0.87\n",
            "[1063 | 2778.24] loss=0.83 avg=0.87\n",
            "[1064 | 2780.69] loss=0.89 avg=0.87\n",
            "[1065 | 2783.14] loss=0.86 avg=0.87\n",
            "Saving checkpoint/run1/model-1065\n",
            "[1066 | 2787.90] loss=0.83 avg=0.87\n",
            "[1067 | 2790.34] loss=0.84 avg=0.87\n",
            "[1068 | 2792.80] loss=0.80 avg=0.87\n",
            "[1069 | 2795.25] loss=0.91 avg=0.87\n",
            "[1070 | 2797.70] loss=0.81 avg=0.87\n",
            "[1071 | 2800.15] loss=0.82 avg=0.87\n",
            "[1072 | 2802.60] loss=0.79 avg=0.87\n",
            "[1073 | 2805.05] loss=0.86 avg=0.87\n",
            "[1074 | 2807.50] loss=0.83 avg=0.87\n",
            "[1075 | 2809.95] loss=0.85 avg=0.87\n",
            "[1076 | 2812.40] loss=0.82 avg=0.87\n",
            "[1077 | 2814.85] loss=0.79 avg=0.87\n",
            "[1078 | 2817.30] loss=0.83 avg=0.87\n",
            "[1079 | 2819.76] loss=0.80 avg=0.87\n",
            "[1080 | 2822.23] loss=0.84 avg=0.87\n",
            "Saving checkpoint/run1/model-1080\n",
            "[1081 | 2826.94] loss=0.81 avg=0.87\n",
            "[1082 | 2829.38] loss=0.83 avg=0.87\n",
            "[1083 | 2831.83] loss=0.83 avg=0.87\n",
            "[1084 | 2834.28] loss=0.83 avg=0.87\n",
            "[1085 | 2836.73] loss=0.91 avg=0.87\n",
            "[1086 | 2839.18] loss=0.82 avg=0.87\n",
            "[1087 | 2841.63] loss=0.88 avg=0.87\n",
            "[1088 | 2844.08] loss=0.84 avg=0.87\n",
            "[1089 | 2846.53] loss=0.83 avg=0.87\n",
            "[1090 | 2848.98] loss=0.85 avg=0.87\n",
            "[1091 | 2851.43] loss=0.88 avg=0.87\n",
            "[1092 | 2853.88] loss=0.83 avg=0.87\n",
            "[1093 | 2856.33] loss=0.86 avg=0.87\n",
            "[1094 | 2858.78] loss=0.83 avg=0.86\n",
            "[1095 | 2861.23] loss=0.84 avg=0.86\n",
            "Saving checkpoint/run1/model-1095\n",
            "[1096 | 2865.90] loss=0.84 avg=0.86\n",
            "[1097 | 2868.34] loss=0.88 avg=0.86\n",
            "[1098 | 2870.79] loss=0.84 avg=0.86\n",
            "[1099 | 2873.24] loss=0.90 avg=0.86\n",
            "[1100 | 2875.69] loss=0.83 avg=0.86\n",
            "======== SAMPLE 1 ========\n",
            "war\n",
            ". I'm not going to lie. I'm\n",
            "The cat is not known.\n",
            "\n",
            "The\n",
            ".S. government has been accused of using the\n",
            " the same cat.\n",
            "\n",
            "\"I'm not\n",
            " was taken to the hospital, where he was pronounced\n",
            " was born in the same cats as the first of\n",
            " that is used to keep the sun's cat.\n",
            " called \"config.json\" and add the following\n",
            "-like creature in the forest.\n",
            "\n",
            "The\n",
            " be\n",
            "\n",
            "[1101 | 2879.04] loss=0.84 avg=0.86\n",
            "[1102 | 2881.50] loss=0.83 avg=0.86\n",
            "[1103 | 2883.97] loss=0.85 avg=0.86\n",
            "[1104 | 2886.41] loss=0.84 avg=0.86\n",
            "[1105 | 2888.87] loss=0.91 avg=0.86\n",
            "[1106 | 2891.32] loss=0.83 avg=0.86\n",
            "[1107 | 2893.77] loss=0.86 avg=0.86\n",
            "[1108 | 2896.22] loss=0.84 avg=0.86\n",
            "[1109 | 2898.67] loss=0.86 avg=0.86\n",
            "[1110 | 2901.12] loss=0.82 avg=0.86\n",
            "Saving checkpoint/run1/model-1110\n",
            "[1111 | 2905.79] loss=0.82 avg=0.86\n",
            "[1112 | 2908.24] loss=0.83 avg=0.86\n",
            "[1113 | 2910.68] loss=0.86 avg=0.86\n",
            "[1114 | 2913.14] loss=0.87 avg=0.86\n",
            "[1115 | 2915.58] loss=0.86 avg=0.86\n",
            "[1116 | 2918.03] loss=0.85 avg=0.86\n",
            "[1117 | 2920.49] loss=0.83 avg=0.86\n",
            "[1118 | 2922.93] loss=0.81 avg=0.86\n",
            "[1119 | 2925.38] loss=0.86 avg=0.86\n",
            "[1120 | 2927.83] loss=0.87 avg=0.86\n",
            "[1121 | 2930.28] loss=0.81 avg=0.86\n",
            "[1122 | 2932.73] loss=0.78 avg=0.86\n",
            "[1123 | 2935.18] loss=0.81 avg=0.86\n",
            "[1124 | 2937.63] loss=0.83 avg=0.86\n",
            "[1125 | 2940.08] loss=0.79 avg=0.86\n",
            "Saving checkpoint/run1/model-1125\n",
            "[1126 | 2944.86] loss=0.81 avg=0.86\n",
            "[1127 | 2947.30] loss=0.82 avg=0.86\n",
            "[1128 | 2949.75] loss=0.86 avg=0.86\n",
            "[1129 | 2952.19] loss=0.82 avg=0.86\n",
            "[1130 | 2954.64] loss=0.89 avg=0.86\n",
            "[1131 | 2957.09] loss=0.77 avg=0.86\n",
            "[1132 | 2959.55] loss=0.81 avg=0.86\n",
            "[1133 | 2962.00] loss=0.83 avg=0.86\n",
            "[1134 | 2964.44] loss=0.83 avg=0.86\n",
            "[1135 | 2966.90] loss=0.86 avg=0.86\n",
            "[1136 | 2969.35] loss=0.84 avg=0.86\n",
            "[1137 | 2971.80] loss=0.84 avg=0.86\n",
            "[1138 | 2974.25] loss=0.80 avg=0.85\n",
            "[1139 | 2976.70] loss=0.82 avg=0.85\n",
            "[1140 | 2979.15] loss=0.85 avg=0.85\n",
            "Saving checkpoint/run1/model-1140\n",
            "[1141 | 2983.98] loss=0.87 avg=0.85\n",
            "[1142 | 2986.42] loss=0.91 avg=0.86\n",
            "[1143 | 2988.87] loss=0.82 avg=0.85\n",
            "[1144 | 2991.32] loss=0.84 avg=0.85\n",
            "[1145 | 2993.77] loss=0.80 avg=0.85\n",
            "[1146 | 2996.22] loss=0.85 avg=0.85\n",
            "[1147 | 2998.68] loss=0.87 avg=0.85\n",
            "[1148 | 3001.13] loss=0.84 avg=0.85\n",
            "[1149 | 3003.58] loss=0.87 avg=0.85\n",
            "[1150 | 3006.03] loss=0.82 avg=0.85\n",
            "[1151 | 3008.50] loss=0.84 avg=0.85\n",
            "[1152 | 3010.95] loss=0.83 avg=0.85\n",
            "[1153 | 3013.40] loss=0.85 avg=0.85\n",
            "[1154 | 3015.85] loss=0.84 avg=0.85\n",
            "[1155 | 3018.30] loss=0.85 avg=0.85\n",
            "Saving checkpoint/run1/model-1155\n",
            "[1156 | 3023.14] loss=0.82 avg=0.85\n",
            "[1157 | 3025.59] loss=0.87 avg=0.85\n",
            "[1158 | 3028.03] loss=0.83 avg=0.85\n",
            "[1159 | 3030.49] loss=0.82 avg=0.85\n",
            "[1160 | 3032.94] loss=0.85 avg=0.85\n",
            "[1161 | 3035.39] loss=0.90 avg=0.85\n",
            "[1162 | 3037.85] loss=0.82 avg=0.85\n",
            "[1163 | 3040.30] loss=0.84 avg=0.85\n",
            "[1164 | 3042.75] loss=0.87 avg=0.85\n",
            "[1165 | 3045.20] loss=0.85 avg=0.85\n",
            "[1166 | 3047.65] loss=0.83 avg=0.85\n",
            "[1167 | 3050.10] loss=0.83 avg=0.85\n",
            "[1168 | 3052.55] loss=0.81 avg=0.85\n",
            "[1169 | 3055.00] loss=0.87 avg=0.85\n",
            "[1170 | 3057.45] loss=0.84 avg=0.85\n",
            "Saving checkpoint/run1/model-1170\n",
            "[1171 | 3062.21] loss=0.90 avg=0.85\n",
            "[1172 | 3064.65] loss=0.89 avg=0.85\n",
            "[1173 | 3067.11] loss=0.88 avg=0.85\n",
            "[1174 | 3069.55] loss=0.82 avg=0.85\n",
            "[1175 | 3072.00] loss=0.79 avg=0.85\n",
            "[1176 | 3074.46] loss=0.82 avg=0.85\n",
            "[1177 | 3076.91] loss=0.82 avg=0.85\n",
            "[1178 | 3079.36] loss=0.84 avg=0.85\n",
            "[1179 | 3081.81] loss=0.84 avg=0.85\n",
            "[1180 | 3084.26] loss=0.88 avg=0.85\n",
            "[1181 | 3086.71] loss=0.82 avg=0.85\n",
            "[1182 | 3089.16] loss=0.81 avg=0.85\n",
            "[1183 | 3091.61] loss=0.80 avg=0.85\n",
            "[1184 | 3094.07] loss=0.85 avg=0.85\n",
            "[1185 | 3096.52] loss=0.85 avg=0.85\n",
            "Saving checkpoint/run1/model-1185\n",
            "[1186 | 3101.28] loss=0.79 avg=0.85\n",
            "[1187 | 3103.72] loss=0.80 avg=0.85\n",
            "[1188 | 3106.18] loss=0.87 avg=0.85\n",
            "[1189 | 3108.63] loss=0.86 avg=0.85\n",
            "[1190 | 3111.08] loss=0.83 avg=0.85\n",
            "[1191 | 3113.53] loss=0.88 avg=0.85\n",
            "[1192 | 3115.98] loss=0.91 avg=0.85\n",
            "[1193 | 3118.43] loss=0.78 avg=0.85\n",
            "[1194 | 3120.88] loss=0.85 avg=0.85\n",
            "[1195 | 3123.33] loss=0.82 avg=0.85\n",
            "[1196 | 3125.78] loss=0.88 avg=0.85\n",
            "[1197 | 3128.23] loss=0.83 avg=0.85\n",
            "[1198 | 3130.68] loss=0.85 avg=0.85\n",
            "[1199 | 3133.13] loss=0.84 avg=0.85\n",
            "[1200 | 3135.58] loss=0.86 avg=0.85\n",
            "Saving checkpoint/run1/model-1200\n",
            "======== SAMPLE 1 ========\n",
            "'s\n",
            " one one one one one one one one one one one\n",
            "\n",
            "\n",
            "The first cats I saw the film was\n",
            " I'm going to do it. I'm going\n",
            " that. I think it's a good cat to\n",
            "The following is a list of the most common cat\n",
            "\n",
            "The first step is to create a new cat\n",
            " been in the care of the Humane cats of the\n",
            "\n",
            "\n",
            "The first cats is to create a new\n",
            " I'm going to do it. I'm going\n",
            ".\n",
            "\n",
            "[1201 | 3141.27] loss=0.82 avg=0.85\n",
            "[1202 | 3143.72] loss=0.88 avg=0.85\n",
            "[1203 | 3146.17] loss=0.87 avg=0.85\n",
            "[1204 | 3148.61] loss=0.83 avg=0.85\n",
            "[1205 | 3151.06] loss=0.83 avg=0.85\n",
            "[1206 | 3153.52] loss=0.84 avg=0.85\n",
            "[1207 | 3155.97] loss=0.81 avg=0.85\n",
            "[1208 | 3158.41] loss=0.83 avg=0.85\n",
            "[1209 | 3160.86] loss=0.81 avg=0.85\n",
            "[1210 | 3163.31] loss=0.89 avg=0.85\n",
            "[1211 | 3165.77] loss=0.86 avg=0.85\n",
            "[1212 | 3168.22] loss=0.81 avg=0.85\n",
            "[1213 | 3170.67] loss=0.85 avg=0.85\n",
            "[1214 | 3173.13] loss=0.81 avg=0.85\n",
            "[1215 | 3175.58] loss=0.82 avg=0.85\n",
            "Saving checkpoint/run1/model-1215\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted: checkpoint/run1/model-1215.data-00000-of-00001.tempstate14992592274422397508; No space left on device\n\t [[{{node save/SaveV2}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[save/SaveV2/_1178]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: checkpoint/run1/model-1215.data-00000-of-00001.tempstate14992592274422397508; No space left on device\n\t [[{{node save/SaveV2}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-a8a7cffe763c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfine_tune_sess1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_fine_tune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-2b889d61891c>\u001b[0m in \u001b[0;36mrun_fine_tune\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m                  \u001b[0monly_train_transformer_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                  \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                  overwrite=False)   # steps is max number of training steps\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'run1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36mfinetune\u001b[0;34m(sess, dataset, steps, model_name, model_dir, combine, batch_size, learning_rate, accumulate_gradients, restore_from, run_name, checkpoint_dir, sample_every, sample_length, sample_num, multi_gpu, save_every, print_every, max_checkpoints, use_memory_saving_gradients, only_train_transformer_layers, optimizer, overwrite)\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msave_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msample_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[0mgenerate_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36msave\u001b[0;34m()\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             global_step=counter-1)\n\u001b[0m\u001b[1;32m    284\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs, save_debug_info)\u001b[0m\n\u001b[1;32m   1174\u001b[0m           model_checkpoint_path = sess.run(\n\u001b[1;32m   1175\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1384\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted: checkpoint/run1/model-1215.data-00000-of-00001.tempstate14992592274422397508; No space left on device\n\t [[node save/SaveV2 (defined at /tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py:1748) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[save/SaveV2/_1178]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: checkpoint/run1/model-1215.data-00000-of-00001.tempstate14992592274422397508; No space left on device\n\t [[node save/SaveV2 (defined at /tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py:1748) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored.\n\nOriginal stack trace for 'save/SaveV2':\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 462, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 492, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 444, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-a8a7cffe763c>\", line 1, in <module>\n    fine_tune_sess1 = run_fine_tune()\n  File \"<ipython-input-5-2b889d61891c>\", line 44, in run_fine_tune\n    overwrite=False)   # steps is max number of training steps\n  File \"/usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py\", line 242, in finetune\n    max_to_keep=max_checkpoints)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py\", line 828, in __init__\n    self.build()\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py\", line 840, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py\", line 878, in _build\n    build_restore=build_restore)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py\", line 505, in _build_internal\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py\", line 206, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py\", line 122, in save_op\n    tensors)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/gen_io_ops.py\", line 1946, in save_v2\n    name=name)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLLXEohDrLjX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}